{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c28e28d1fef919",
   "metadata": {},
   "source": [
    "# Parquet konvertering-, partisjonering- og filtreringsverktøy for AIS-data\n",
    "\n",
    "Dette verktøyet konverterer Parquet-filer til GeoParquet-format, partisjonerer på angitte kolonner og filtrerer på angitt dato og radius rundt Kristiansand sentrum. Data vises på tabell og kart.\n",
    "\n",
    "Verktøyet er en løsningsmodell som bruker GeoPandas for følgende brukerhistorie:\n",
    "\n",
    "\"Som havnesjef i Kristiansand kommune ønsker jeg å vite hvilke skip som befant seg innen 6 km fra Kristiansand sentrum på en bestemt dato.\"\n",
    "\n",
    "### Hvordan bruke dette verktøyet\n",
    "1. Last opp dine geografiske datafiler til mappen `data/raw`\n",
    "2. Kjør alle cellene i denne notebooken\n",
    "3. De konverterte filene vil bli lagret i mappen `data/processed`\n",
    "\n",
    "### Støttede filformater\n",
    "\n",
    "- Parquet-filer med geografisk informasjon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff99a21e-c60f-47e3-bebf-1698fb9da7ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global variabel for mappestier\n",
    "_current_observer = None\n",
    "global_paths = None\n",
    "\n",
    "# Sett standardverdier for parameterne\n",
    "data_dir = \"./data\"\n",
    "num_files = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a56c74f7824305",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T17:35:41.085444Z",
     "start_time": "2025-04-17T17:35:41.080570Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "\n",
    "from shapely import wkb\n",
    "from shapely.geometry import Point, Polygon\n",
    "import time\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import folium\n",
    "from folium import plugins\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "from functools import partial\n",
    "\n",
    "def find_newest_folder(folder_path, use_foldername=True):\n",
    "    \"\"\"\n",
    "    Finner den nyeste mappen i en mappe basert på mappenavnet (dato) eller modifiseringstidspunkt.\n",
    "    Leter etter mappenavn på formatet: hais_2024-12-24.snappy.parquet\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Stien til mappen hvor undermappene skal søkes\n",
    "        use_foldername (bool): Om dato i mappenavn skal brukes (True) eller sist endret tid (False)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (str: Stien til den nyeste mappen, str: Datoen som string i format 'YYYY-MM-DD',\n",
    "               eller None hvis det ikke finnes mapper eller dato ikke ble funnet)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Sjekk om mappen eksisterer\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Mappen {folder_path} eksisterer ikke\")\n",
    "            return None, None\n",
    "\n",
    "        # Finn alle mapper i mappen (ikke filer)\n",
    "        folders = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "                  if os.path.isdir(os.path.join(folder_path, f))]\n",
    "\n",
    "        if not folders:\n",
    "            print(f\"Ingen mapper funnet i {folder_path}\")\n",
    "            return None, None\n",
    "\n",
    "        if use_foldername:\n",
    "            # Finn den nyeste mappen basert på dato i mappenavnet\n",
    "            newest_date = None\n",
    "            newest_folder = None\n",
    "            date_string = None\n",
    "\n",
    "            for folder in folders:\n",
    "                foldername = os.path.basename(folder)\n",
    "                # Søk etter dato i formatet YYYY-MM-DD i mappenavnet\n",
    "                date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', foldername)\n",
    "\n",
    "                if date_match:\n",
    "                    date_str = date_match.group(1)\n",
    "                    try:\n",
    "                        folder_date = datetime.datetime.strptime(date_str, '%Y-%m-%d').date()\n",
    "\n",
    "                        if newest_date is None or folder_date > newest_date:\n",
    "                            newest_date = folder_date\n",
    "                            newest_folder = folder\n",
    "                            date_string = date_str\n",
    "                    except ValueError:\n",
    "                        # Hvis datoen ikke kan tolkes, hopp over denne mappen\n",
    "                        continue\n",
    "\n",
    "            if newest_folder:\n",
    "                return newest_folder, date_string\n",
    "            else:\n",
    "                print(\"Ingen mapper med gyldig datoformat funnet. Bruker modifiseringstidspunkt i stedet.\")\n",
    "                # Fall tilbake til modifiseringstidspunkt\n",
    "                newest_folder = max(folders, key=os.path.getmtime)\n",
    "                # Ingen dato funnet i mappenavn, så vi returnerer None som date_string\n",
    "                return newest_folder, None\n",
    "        else:\n",
    "            # Finn den nyeste mappen basert på modifiseringstidspunkt\n",
    "            newest_folder = max(folders, key=os.path.getmtime)\n",
    "\n",
    "            # Sjekk om vi kan finne en dato i mappenavnet til den nyeste mappen\n",
    "            foldername = os.path.basename(newest_folder)\n",
    "            date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', foldername)\n",
    "            date_string = date_match.group(1) if date_match else None\n",
    "\n",
    "            return newest_folder, date_string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Feil ved søk etter nyeste mappe: {str(e)}\")\n",
    "        return None, None\n",
    "    \n",
    "def create_folders(data_folder=\"./data\"):\n",
    "    \"\"\"\n",
    "    Oppretter nødvendig mappestruktur for konvertering.\n",
    "\n",
    "    Args:\n",
    "        data_folder: Sti til hovedmappen for data\n",
    "\n",
    "    Returns:\n",
    "        dict: Stier til opprettede mapper\n",
    "    \"\"\"\n",
    "    # Definer mappestruktur\n",
    "    raw_folder = os.path.join(data_folder, \"raw\")\n",
    "    processed_folder = os.path.join(data_folder, \"processed\")\n",
    "    source_folder = os.path.join(data_folder, \"source\")\n",
    "    incoming_folder = os.path.join(data_folder, \"incoming\")\n",
    "\n",
    "    # Opprett mapper\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    os.makedirs(raw_folder, exist_ok=True)\n",
    "    os.makedirs(processed_folder, exist_ok=True)\n",
    "    os.makedirs(source_folder, exist_ok=True)\n",
    "    os.makedirs(incoming_folder, exist_ok=True)\n",
    "\n",
    "    # Returner stier for senere bruk\n",
    "    return {\n",
    "        \"data_folder\": os.path.abspath(data_folder),\n",
    "        \"raw_folder\": os.path.abspath(raw_folder),\n",
    "        \"processed_folder\": os.path.abspath(processed_folder),\n",
    "        \"source_folder\": os.path.abspath(source_folder),\n",
    "        \"incoming_folder\": os.path.abspath(incoming_folder)\n",
    "    }\n",
    "\n",
    "def convert_parquet_to_geoparquet(file_path, output_path, partition_columns=None):\n",
    "    \"\"\"\n",
    "    Konverterer parquet-fil til GeoParquet-format med partisjonering.\n",
    "    Args:\n",
    "        file_path: Sti til parquet-filen\n",
    "        output_path: Sti hvor GeoParquet-filen skal lagres\n",
    "        partition_columns: Liste av kolonnenavn som skal brukes for partisjonering\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Kunne ikke lese parquet-fil: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Håndter tidspartisjonering\n",
    "    time_partitioning = False\n",
    "    if partition_columns and 'date_time_utc' in partition_columns:\n",
    "        partition_columns.remove('date_time_utc')\n",
    "        time_partitioning = True\n",
    "\n",
    "    # Legg til alle tidspartisjoneringskolonner, men vi vil bare partisjonere på time\n",
    "    if time_partitioning:\n",
    "        df = add_time_partitioning_columns(df)\n",
    "        # Sett opp partisjonering kun på 'hour'\n",
    "        partition_columns = ['hour'] + (partition_columns or [])\n",
    "\n",
    "    # Opprett GeoDataFrame\n",
    "    gdf = create_geodataframe(df)\n",
    "    if gdf is None:\n",
    "        return False\n",
    "\n",
    "    # Sjekk at alle partisjoneringskolonner finnes\n",
    "    if partition_columns and not all(col in gdf.columns for col in partition_columns):\n",
    "        print(f\"Advarsel: Ikke alle partisjoneringskolonner finnes i datasettet\")\n",
    "        missing = [col for col in partition_columns if col not in gdf.columns]\n",
    "        print(f\"Manglende kolonner: {missing}\")\n",
    "        return False\n",
    "\n",
    "    # Lagre med partisjonering (bare time)\n",
    "    if partition_columns:\n",
    "        return save_partitioned_geoparquet(gdf, output_path, partition_columns)\n",
    "    else:\n",
    "        gdf.to_parquet(output_path)\n",
    "        return output_path\n",
    "    \n",
    "def add_time_partitioning_columns(df, time_column='date_time_utc'):\n",
    "    \"\"\"\n",
    "    Legger til kolonner for tidspartisjonering (kun time for partisjonering, men beholder dato-kolonner).\n",
    "    \"\"\"\n",
    "    if time_column not in df.columns:\n",
    "        print(f\"Advarsel: Tidsstempelkolonne '{time_column}' finnes ikke\")\n",
    "        return df\n",
    "\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[time_column]):\n",
    "        print(f\"Advarsel: Kolonnen '{time_column}' er ikke en datetime-kolonne\")\n",
    "        return df\n",
    "\n",
    "    df_with_time = df.copy()\n",
    "    df_with_time['year'] = df_with_time[time_column].dt.year\n",
    "    df_with_time['month'] = df_with_time[time_column].dt.month\n",
    "    df_with_time['day'] = df_with_time[time_column].dt.day\n",
    "    df_with_time['hour'] = df_with_time[time_column].dt.hour\n",
    "\n",
    "    return df_with_time\n",
    "\n",
    "def create_geodataframe(df):\n",
    "    \"\"\"\n",
    "    Oppretter en GeoDataFrame fra en DataFrame ved å finne koordinater eller geometrikolonner.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sjekk for lat/long kolonner\n",
    "    if 'longitude' in df.columns and 'latitude' in df.columns:\n",
    "        try:\n",
    "            return gpd.GeoDataFrame(\n",
    "                df,\n",
    "                geometry=gpd.points_from_xy(df.longitude, df.latitude),\n",
    "                crs=\"EPSG:4326\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Kunne ikke opprette geometri fra lat/long: {e}\")\n",
    "\n",
    "    # Sjekk for andre geometrikolonner\n",
    "    geom_columns = [col for col in df.columns if any(\n",
    "        term in col.lower() for term in ['geom', 'coord', 'point', 'polygon', 'linestring', 'wkt']\n",
    "    )]\n",
    "\n",
    "    for col in geom_columns:\n",
    "        if df[col].dtype != 'object':\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            geom = df[col].apply(wkt.loads)\n",
    "            return gpd.GeoDataFrame(df, geometry=geom, crs=\"EPSG:4326\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_partitioned_geoparquet(gdf, output_path, partition_columns):\n",
    "    \"\"\"\n",
    "    Lagrer en GeoDataFrame som partisjonert GeoParquet.\n",
    "    \"\"\"\n",
    "\n",
    "    # Konverterer geometri til WKB for å kunne partisjonere\n",
    "    df_med_wkb = gdf.copy()\n",
    "    df_med_wkb['geometry_wkb'] = df_med_wkb['geometry'].apply(lambda geom: wkb.dumps(geom))\n",
    "    df_for_partisjon = df_med_wkb.drop(columns=['geometry'])\n",
    "\n",
    "    # Utfør partisjonering\n",
    "    df_for_partisjon.to_parquet(output_path, partition_cols=partition_columns)\n",
    "\n",
    "    # Konverter hver partisjonert fil tilbake til GeoParquet\n",
    "    convert_partitioned_files_to_geoparquet(output_path)\n",
    "    return output_path\n",
    "\n",
    "def convert_partitioned_files_to_geoparquet(root_folder):\n",
    "    \"\"\"\n",
    "    Konverterer alle partisjonerte parquet-filer til GeoParquet format.\n",
    "    \"\"\"\n",
    "    error_count = 0\n",
    "\n",
    "    for root, _, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if not file.endswith('.parquet'):\n",
    "                continue\n",
    "\n",
    "            parquet_path = os.path.join(root, file)\n",
    "            try:\n",
    "                # Les dataframe\n",
    "                part_df = pd.read_parquet(parquet_path)\n",
    "\n",
    "                # Hopp over hvis den ikke har geometry_wkb\n",
    "                if 'geometry_wkb' not in part_df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Konverter WKB tilbake til geometri\n",
    "                part_df['geometry'] = part_df['geometry_wkb'].apply(lambda x: wkb.loads(x))\n",
    "                part_df = part_df.drop(columns=['geometry_wkb'])\n",
    "\n",
    "                # Lag GeoDataFrame\n",
    "                part_gdf = gpd.GeoDataFrame(part_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "                # Skriv GeoParquet-filen\n",
    "                part_gdf.to_parquet(parquet_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Feil ved konvertering av {parquet_path}: {e}\")\n",
    "                error_count += 1\n",
    "\n",
    "    if error_count > 0:\n",
    "        print(f\"Advarsel: {error_count} filer kunne ikke konverteres til GeoParquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f2b8e5679dbc78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T17:35:42.908105Z",
     "start_time": "2025-04-17T17:35:42.903652Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/processed\\\\hais_2024-11-05.snappy.parquet', '2024-11-05')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = \"data/processed\"\n",
    "find_newest_folder(folder_path, use_foldername=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba0acb-5354-4d2b-bd66-4ab8a5079fa1",
   "metadata": {},
   "source": [
    "## Strømming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd4fa8d2-4b17-4f4f-a20a-6eef8118b0f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## ----- STRØMMING -----\n",
    "\n",
    "# Definisjon av FileSystemEventHandler for å håndtere nye filer\n",
    "class GeoDataHandler(FileSystemEventHandler):\n",
    "    def __init__(self, incoming_dir, raw_dir, processed_dir, source_dir):\n",
    "        self.incoming_dir = incoming_dir\n",
    "        self.raw_dir = raw_dir\n",
    "        self.processed_dir = processed_dir\n",
    "        self.source_dir = source_dir\n",
    "\n",
    "    def on_created(self, event):\n",
    "        # Vi er kun interessert i filhendelser (ikke mappeopprettelser)\n",
    "        if not event.is_directory:\n",
    "            filepath = event.src_path\n",
    "            filename = os.path.basename(filepath)\n",
    "\n",
    "            # Sjekk om det er en parquet-fil\n",
    "            if filename.endswith('.parquet'):\n",
    "                print(f\"Oppdaget ny fil: {filename}\")\n",
    "\n",
    "                # Vent litt for å sikre at filen er ferdig skrevet\n",
    "                time.sleep(1)\n",
    "\n",
    "                # Flytt filen til raw-mappen\n",
    "                destination = os.path.join(self.raw_dir, filename)\n",
    "                shutil.move(filepath, destination)\n",
    "\n",
    "                # Prosesser filen\n",
    "                self.process_file(destination)\n",
    "\n",
    "    def process_file(self, filepath):\n",
    "        try:\n",
    "            # Definer målsti for den konverterte filen\n",
    "            base_filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "            output_path = os.path.join(self.processed_dir, f\"{base_filename}\")\n",
    "\n",
    "            # Konverter parquet til GeoParquet med time-partisjonering\n",
    "            print(f\"Konverterer og partisjonerer fil...\")\n",
    "            results = convert_parquet_to_geoparquet(\n",
    "                filepath,\n",
    "                output_path,\n",
    "                partition_columns=[\"date_time_utc\"]  # Vil bruke time-partisjonering\n",
    "            )\n",
    "\n",
    "            if results:\n",
    "                print(f\"Konvertering og partisjonering vellykket.\")\n",
    "                # INGEN ARKIVERING TIL SOURCE-MAPPEN HER\n",
    "            else:\n",
    "                print(f\"Konvertering feilet for {filepath}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Feil ved prosessering av {filepath}: {str(e)}\")\n",
    "            # Logge feilen for senere analyse\n",
    "            with open(os.path.join(os.path.dirname(self.processed_dir), \"error.log\"), \"a\") as log_file:\n",
    "                timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                log_file.write(f\"{timestamp} - {filepath}: {str(e)}\\n\")\n",
    "\n",
    "def start_monitoring(data_folder=\"./data\"):\n",
    "    global _current_observer, global_paths\n",
    "\n",
    "    # Stopp eksisterende observer hvis den finnes\n",
    "    if _current_observer is not None:\n",
    "        _current_observer.stop()\n",
    "        _current_observer.join()\n",
    "        print(\"Stoppet eksisterende overvåking.\")\n",
    "\n",
    "    # Gjenbruk eksisterende mappestruktur eller opprett en ny\n",
    "    if global_paths is None:\n",
    "        global_paths = create_folders(data_folder)\n",
    "\n",
    "    # Opprett event handler med den NYE GeoDataHandler-klassen\n",
    "    event_handler = GeoDataHandler(\n",
    "        global_paths[\"incoming_folder\"],\n",
    "        global_paths[\"raw_folder\"],\n",
    "        global_paths[\"processed_folder\"],\n",
    "        global_paths[\"source_folder\"]\n",
    "    )\n",
    "\n",
    "    # Start observer\n",
    "    _current_observer = Observer()\n",
    "    _current_observer.schedule(event_handler, global_paths[\"incoming_folder\"], recursive=False)\n",
    "    _current_observer.start()\n",
    "\n",
    "    print(f\"Starter overvåking av mappen: {global_paths['incoming_folder']}\")\n",
    "\n",
    "    return _current_observer\n",
    "\n",
    "def start_notebook_monitoring():\n",
    "    print(\"Starter overvåking...\")\n",
    "    observer = start_monitoring(data_dir)\n",
    "    print(\"Observer startet. Kjør stop_monitoring() i en annen celle for å stoppe.\")\n",
    "    return observer\n",
    "\n",
    "def stop_monitoring():\n",
    "    global _current_observer\n",
    "    if _current_observer is not None:\n",
    "        _current_observer.stop()\n",
    "        _current_observer.join()\n",
    "        _current_observer = None\n",
    "\n",
    "def stop_notebook_monitoring():\n",
    "    global notebook_observer\n",
    "    if notebook_observer is not None:\n",
    "        stop_monitoring()\n",
    "        notebook_observer = None\n",
    "        print(\"Overvåking stoppet.\")\n",
    "    else:\n",
    "        print(\"Ingen aktiv overvåking å stoppe.\")\n",
    "\n",
    "def simulate_streaming(data_folder=\"./data\", num_files=5, min_interval=2, max_interval=10):\n",
    "    global global_paths\n",
    "\n",
    "    # Gjenbruk eksisterende mappestruktur eller opprett en ny\n",
    "    if global_paths is None:\n",
    "        global_paths = create_folders(data_folder)\n",
    "\n",
    "    source_dir = global_paths[\"source_folder\"]\n",
    "    incoming_dir = global_paths[\"incoming_folder\"]\n",
    "\n",
    "    # Finn alle parquet-filer i kildemappen\n",
    "    files = glob.glob(os.path.join(source_dir, \"*.parquet\"))\n",
    "\n",
    "    if not files:\n",
    "        print(f\"Ingen .parquet-filer funnet i {source_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Fant {len(files)} nye filer. Starter simulert strømming...\")\n",
    "\n",
    "    # Begrens antall filer til num_files eller antall tilgjengelige filer\n",
    "    files_to_process = min(len(files), num_files)\n",
    "\n",
    "    #for i in range(files_to_process):\n",
    "    i = 0\n",
    "    for file in files:\n",
    "        # Velg en tilfeldig fil\n",
    "        #file_idx = random.randint(0, len(files)-1)\n",
    "        #file = files[i]\n",
    "\n",
    "        # Generer et tilfeldig tidsintervall\n",
    "        wait_time = random.uniform(min_interval, max_interval)\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "        filename = f\"{i+1}_{os.path.basename(file)}\"  # Legg til indeks for å unngå duplikater\n",
    "        destination = os.path.join(incoming_dir, filename)\n",
    "        print(f\"Destination {destination}\")\n",
    "\n",
    "        # Kopier filen (ikke flytt, så vi kan bruke den flere ganger)\n",
    "        shutil.move(file, destination)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bea66b-c9ac-4cb7-bcae-92fb83d32f4b",
   "metadata": {},
   "source": [
    "## Kjør denne for å starte overvåkingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9015128-6792-4d7e-a54e-f9c9f528619c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starter overvåking...\n",
      "Starter overvåking av mappen: C:\\Users\\esper18\\CodeProjects\\CN\\kaidata_geolake\\Christine\\data\\incoming\n",
      "Observer startet. Kjør stop_monitoring() i en annen celle for å stoppe.\n"
     ]
    }
   ],
   "source": [
    "notebook_observer = start_notebook_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041460b1-72b1-49ec-8792-c23300c79e1a",
   "metadata": {},
   "source": [
    "## Kjør denne funksjonen for å stoppe overvåking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77f0cf56-8423-437b-9f11-5c7c9778544c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overvåking stoppet.\n"
     ]
    }
   ],
   "source": [
    "stop_notebook_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647c15c-ec1e-49c0-ae5d-2c2673b6217e",
   "metadata": {},
   "source": [
    "## Denne cellen kan kjøres for å simulere strømming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f508f152-1609-4767-8563-86abe7fa27d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fant 3 nye filer. Starter simulert strømming...\n",
      "Destination C:\\Users\\esper18\\CodeProjects\\CN\\kaidata_geolake\\Christine\\data\\incoming\\1_hais_2024-10-01.snappy.parquet\n",
      "Oppdaget ny fil: 1_hais_2024-10-01.snappy.parquet\n",
      "Konverterer og partisjonerer fil...\n",
      "Konvertering og partisjonering vellykket.\n",
      "Destination C:\\Users\\esper18\\CodeProjects\\CN\\kaidata_geolake\\Christine\\data\\incoming\\2_hais_2024-10-24.snappy.parquet\n",
      "Oppdaget ny fil: 2_hais_2024-10-24.snappy.parquet\n",
      "Konverterer og partisjonerer fil...\n",
      "Konvertering og partisjonering vellykket.\n",
      "Destination C:\\Users\\esper18\\CodeProjects\\CN\\kaidata_geolake\\Christine\\data\\incoming\\3_hais_2024-11-05.snappy.parquet\n",
      "Oppdaget ny fil: 3_hais_2024-11-05.snappy.parquet\n",
      "Simulering fullført.\n",
      "Konverterer og partisjonerer fil...\n",
      "Konvertering og partisjonering vellykket.\n"
     ]
    }
   ],
   "source": [
    "def start_notebook_simulation():\n",
    "\n",
    "    # Sjekk om source-mappen inneholder filer\n",
    "    source_files = glob.glob(os.path.join(global_paths['source_folder'], \"*.parquet\"))\n",
    "\n",
    "    simulate_streaming(data_dir, num_files)\n",
    "    print(\"Simulering fullført.\")\n",
    "\n",
    "start_notebook_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9827fdec-b06e-4610-859c-4218c6fd1acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd732a0-2c42-4fde-a113-60a97bcd58ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
