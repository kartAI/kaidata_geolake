{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581ad91ceb0f2708",
   "metadata": {},
   "source": [
    "# Demo for styringsmøte den 10.03.2025\n",
    "\n",
    "NB! må kjøre konverting av filer seksjonen først\n",
    "\n",
    "## Importering av pakke"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T12:25:59.384208Z",
     "start_time": "2025-03-07T12:25:59.378129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import time as time_module\n",
    "from shapely.geometry import box\n",
    "from IPython.display import display\n",
    "import folium\n",
    "import re\n"
   ],
   "id": "8643f97bb8fba72",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Konstante variabler",
   "id": "f6278f3749eaa16a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T12:19:24.249028Z",
     "start_time": "2025-03-07T12:19:24.243354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ROTMAPPE = \"Data\"\n",
    "PARTISJON_MAPPE_HOUR = f\"{ROTMAPPE}/partitioned_hour\"\n",
    "PARTISJON_MAPPE_MIN = f\"{ROTMAPPE}/partitioned_minutes\""
   ],
   "id": "147bd22c040c302d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Konverting av filer",
   "id": "c0a5c521a45c637f"
  },
  {
   "cell_type": "code",
   "id": "074a2441",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T12:13:29.961488Z",
     "start_time": "2025-03-07T12:13:29.952729Z"
    }
   },
   "source": [
    "def er_geoparquet(filsti):\n",
    "    \"\"\"Sjekker om en parquet-fil er en GeoParquet-fil\"\"\"\n",
    "    try:\n",
    "        # Les metadata fra parquet-filen\n",
    "        metadata = pq.read_metadata(filsti)\n",
    "        \n",
    "        # Sjekk om metadata inneholder noen geometri-kolonner\n",
    "        schema = pq.read_schema(filsti)\n",
    "        for navn in schema.names:\n",
    "            felt = schema.field(navn)\n",
    "            # Sjekk metadata for kolonnen\n",
    "            if felt.metadata and ('geo' in felt.metadata or b'geo' in felt.metadata):\n",
    "                return True\n",
    "                \n",
    "        # Les filen med geopandas og se om den inneholder en geometrikolonne\n",
    "        try:\n",
    "            gdf = gpd.read_parquet(filsti)\n",
    "            if hasattr(gdf, 'geometry') and gdf.geometry.name in gdf.columns:\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "        return False\n",
    "    except Exception:\n",
    "        return False"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T12:13:30.008155Z",
     "start_time": "2025-03-07T12:13:29.987631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def konverter_til_geoparquet(kildefilsti, målmappesti=None):\n",
    "    \"\"\"Konverterer ulike geografiske formater til GeoParquet-format\"\"\"\n",
    "    try:\n",
    "        # Få filnavn uten endelse\n",
    "        base_navn = os.path.basename(kildefilsti)\n",
    "        base_navn_uten_endelse = os.path.splitext(base_navn)[0]\n",
    "        filendelse = os.path.splitext(kildefilsti)[1].lower()\n",
    "        \n",
    "        # Bestem målfilsti\n",
    "        if målmappesti is None:\n",
    "            # Hvis ingen målmappe er angitt, bruk samme mappe med _geo.parquet\n",
    "            dir_navn = os.path.dirname(kildefilsti)\n",
    "            målfilsti = os.path.join(dir_navn, f\"{base_navn_uten_endelse}_geo.parquet\")\n",
    "        else:\n",
    "            # Hvis målmappe er angitt, plasser filen der\n",
    "            målfilsti = os.path.join(målmappesti, f\"{base_navn_uten_endelse}.parquet\")\n",
    "        \n",
    "        # Sørg for at målmappen eksisterer\n",
    "        os.makedirs(os.path.dirname(målfilsti), exist_ok=True)\n",
    "        \n",
    "        gdf = None\n",
    "        \n",
    "        # Behandle filen basert på filtype\n",
    "        if filendelse == '.parquet':\n",
    "            # Sjekk først om filen er en vanlig parquet-fil\n",
    "            df = pd.read_parquet(kildefilsti)\n",
    "            \n",
    "            # Sjekk etter lat/long kolonner\n",
    "            if 'longitude' in df.columns and 'latitude' in df.columns:\n",
    "                gdf = gpd.GeoDataFrame(\n",
    "                    df, \n",
    "                    geometry=gpd.points_from_xy(df.longitude, df.latitude),\n",
    "                    crs=\"EPSG:4326\"\n",
    "                )\n",
    "            \n",
    "            # Sjekk etter potensielle geometrikolonner\n",
    "            if gdf is None:\n",
    "                geom_kolonner = [col for col in df.columns if 'geom' in col.lower() \n",
    "                              or 'coord' in col.lower() \n",
    "                              or 'point' in col.lower() \n",
    "                              or 'polygon' in col.lower()\n",
    "                              or 'linestring' in col.lower()\n",
    "                              or 'wkt' in col.lower()]\n",
    "                \n",
    "                for col in geom_kolonner:\n",
    "                    try:\n",
    "                        # Prøv å konvertere fra WKT-format hvis det er en tekstkolonne\n",
    "                        if df[col].dtype == 'object':\n",
    "                            from shapely import wkt\n",
    "                            geom = df[col].apply(wkt.loads)\n",
    "                            gdf = gpd.GeoDataFrame(df, geometry=geom, crs=\"EPSG:4326\")\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        \n",
    "        elif filendelse in ['.geojson', '.json']:\n",
    "            # Les GeoJSON-filen uten å spesifisere driver for å unngå advarsel\n",
    "            gdf = gpd.read_file(kildefilsti)\n",
    "            \n",
    "        elif filendelse == '.shp':\n",
    "            # Les shapefile\n",
    "            gdf = gpd.read_file(kildefilsti)\n",
    "            \n",
    "        elif filendelse == '.gpkg':\n",
    "            # Les GeoPackage\n",
    "            gdf = gpd.read_file(kildefilsti, driver='GPKG')\n",
    "            \n",
    "        elif filendelse == '.csv':\n",
    "            # Prøv å lese CSV med lat/long kolonner\n",
    "            df = pd.read_csv(kildefilsti)\n",
    "            \n",
    "            # Sjekk for vanlige lat/long kolonnenavn\n",
    "            lat_kolonner = ['latitude', 'lat', 'y', 'breddegrad']\n",
    "            lon_kolonner = ['longitude', 'long', 'lon', 'x', 'lengdegrad']\n",
    "            \n",
    "            lat_col = next((col for col in lat_kolonner if col in df.columns), None)\n",
    "            lon_col = next((col for col in lon_kolonner if col in df.columns), None)\n",
    "            \n",
    "            if lat_col and lon_col:\n",
    "                gdf = gpd.GeoDataFrame(\n",
    "                    df, \n",
    "                    geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
    "                    crs=\"EPSG:4326\"\n",
    "                )\n",
    "        \n",
    "        # Hvis vi har en gyldig geodataframe, lagre som GeoParquet\n",
    "        if gdf is not None:\n",
    "            gdf.to_parquet(målfilsti)\n",
    "            return målfilsti\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Feil ved konvertering av {kildefilsti}: {e}\")\n",
    "        return False"
   ],
   "id": "453979dd35596cd4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T12:13:30.041743Z",
     "start_time": "2025-03-07T12:13:30.025644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def finn_nye_filer(rå_mappesti, prosessert_mappesti, konverteringslogg_sti=None):\n",
    "    \"\"\"Finner filer som ikke har blitt konvertert ennå\"\"\"\n",
    "    # Støttede filformater\n",
    "    støttede_formater = ['.parquet', '.geojson', '.json', '.shp', '.gpkg', '.csv']\n",
    "    \n",
    "    # Hent alle allerede konverterte filer\n",
    "    konverterte_filer = set()\n",
    "    if os.path.exists(prosessert_mappesti):\n",
    "        for root, _, filer in os.walk(prosessert_mappesti):\n",
    "            for fil in filer:\n",
    "                if fil.endswith('.parquet'):\n",
    "                    base_navn = os.path.splitext(fil)[0]\n",
    "                    konverterte_filer.add(base_navn)\n",
    "    \n",
    "    # Last inn konverteringsloggen hvis den finnes\n",
    "    konverterte_logger = {}\n",
    "    if konverteringslogg_sti and os.path.exists(konverteringslogg_sti):\n",
    "        try:\n",
    "            with open(konverteringslogg_sti, 'r') as f:\n",
    "                konverterte_logger = json.load(f)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Finn alle filer som må konverteres\n",
    "    nye_filer = []\n",
    "    for root, _, filer in os.walk(rå_mappesti):\n",
    "        for fil in filer:\n",
    "            filsti = os.path.join(root, fil)\n",
    "            filendelse = os.path.splitext(fil)[1].lower()\n",
    "            base_navn = os.path.splitext(fil)[0]\n",
    "            \n",
    "            # Sjekk om filen har et støttet format\n",
    "            if filendelse in støttede_formater:\n",
    "                # Parquet-filer trenger en ekstra sjekk\n",
    "                if filendelse == '.parquet':\n",
    "                    # Ikke ta med filer som allerede er GeoParquet\n",
    "                    if er_geoparquet(filsti):\n",
    "                        continue\n",
    "                    # Ikke ta med filer som allerede har _geo suffix\n",
    "                    if base_navn.endswith('_geo'):\n",
    "                        continue\n",
    "                \n",
    "                # Sjekk om filen allerede er konvertert\n",
    "                if base_navn not in konverterte_filer:\n",
    "                    # Sjekk om filen har endret seg siden sist konvertering\n",
    "                    if base_navn in konverterte_logger:\n",
    "                        sist_endret_tid = os.path.getmtime(filsti)\n",
    "                        sist_konvertert_tid = konverterte_logger[base_navn].get('tidspunkt', 0)\n",
    "                        if sist_endret_tid <= sist_konvertert_tid:\n",
    "                            continue\n",
    "                    \n",
    "                    nye_filer.append(filsti)\n",
    "    \n",
    "    return nye_filer"
   ],
   "id": "1ff928da1d3bb59c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T12:13:30.066707Z",
     "start_time": "2025-03-07T12:13:30.059836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def opprett_mappestruktur(rotmappe):\n",
    "    \"\"\"Oppretter den anbefalte mappestrukturen\"\"\"\n",
    "    mapper = [\n",
    "        os.path.join(rotmappe, 'raw'),\n",
    "        os.path.join(rotmappe, 'processed'),\n",
    "        os.path.join(rotmappe, 'archive')\n",
    "    ]\n",
    "    \n",
    "    for mappe in mapper:\n",
    "        os.makedirs(mappe, exist_ok=True)\n",
    "    \n",
    "    return {\n",
    "        'raw': mapper[0],\n",
    "        'processed': mapper[1],\n",
    "        'archive': mapper[2]\n",
    "    }"
   ],
   "id": "8a47ebbb3af5e15f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T12:13:30.094392Z",
     "start_time": "2025-03-07T12:13:30.083855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def oppdater_konverteringslogg(konverteringslogg_sti, fil, målfilsti):\n",
    "    \"\"\"Oppdaterer konverteringsloggen med ny filinfo\"\"\"\n",
    "    konverterte_logger = {}\n",
    "    \n",
    "    # Last inn eksisterende logg hvis den finnes\n",
    "    if os.path.exists(konverteringslogg_sti):\n",
    "        try:\n",
    "            with open(konverteringslogg_sti, 'r') as f:\n",
    "                konverterte_logger = json.load(f)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Legg til ny informasjon\n",
    "    base_navn = os.path.splitext(os.path.basename(fil))[0]\n",
    "    konverterte_logger[base_navn] = {\n",
    "        'kildesti': fil,\n",
    "        'målsti': målfilsti,\n",
    "        'tidspunkt': datetime.datetime.now().timestamp(),\n",
    "        'dato': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    # Lagre oppdatert logg\n",
    "    with open(konverteringslogg_sti, 'w') as f:\n",
    "        json.dump(konverterte_logger, f, indent=2)"
   ],
   "id": "2766de870fb052fc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T12:13:30.139309Z",
     "start_time": "2025-03-07T12:13:30.117181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def behandle_alle_filer_i_mappe(rotmappesti, opprett_struktur=True):\n",
    "    \"\"\"Behandler alle geografiske filer i angitt mappe og konverterer til GeoParquet.\"\"\"\n",
    "    # Sjekk om katalogen eksisterer\n",
    "    if not os.path.exists(rotmappesti):\n",
    "        print(f\"FEIL: Katalogen '{rotmappesti}' eksisterer ikke.\")\n",
    "        return None\n",
    "    \n",
    "    mappestruktur = None\n",
    "    rå_mappesti = rotmappesti\n",
    "    prosessert_mappesti = rotmappesti\n",
    "    \n",
    "    # Opprett mappestruktur hvis ønsket\n",
    "    if opprett_struktur:\n",
    "        mappestruktur = opprett_mappestruktur(rotmappesti)\n",
    "        rå_mappesti = mappestruktur['raw']\n",
    "        prosessert_mappesti = mappestruktur['processed']\n",
    "        print(f\"Opprettet mappestruktur:\\n- Raw: {rå_mappesti}\\n- Processed: {prosessert_mappesti}\")\n",
    "    \n",
    "    # Konverteringslogg-sti\n",
    "    konverteringslogg_sti = os.path.join(rotmappesti, 'conversion_log.json')\n",
    "    \n",
    "    # Finn nye filer som trenger konvertering\n",
    "    nye_filer = finn_nye_filer(rå_mappesti, prosessert_mappesti, konverteringslogg_sti)\n",
    "    \n",
    "    resultater = {\n",
    "        \"allerede_geoparquet\": [],\n",
    "        \"konvertert\": [],\n",
    "        \"feilet\": [],\n",
    "        \"alle_filer\": []\n",
    "    }\n",
    "    \n",
    "    # Konverter filer\n",
    "    for filsti in nye_filer:\n",
    "        resultater[\"alle_filer\"].append(filsti)\n",
    "        \n",
    "        # Konverter filen\n",
    "        målfilsti = konverter_til_geoparquet(filsti, prosessert_mappesti)\n",
    "        \n",
    "        if målfilsti:\n",
    "            resultater[\"konvertert\"].append(filsti)\n",
    "            oppdater_konverteringslogg(konverteringslogg_sti, filsti, målfilsti)\n",
    "        else:\n",
    "            resultater[\"feilet\"].append(filsti)\n",
    "    \n",
    "    # Finn allerede eksisterende GeoParquet-filer\n",
    "    for root, _, filer in os.walk(rå_mappesti):\n",
    "        for fil in filer:\n",
    "            if fil.endswith('.parquet'):\n",
    "                filsti = os.path.join(root, fil)\n",
    "                if er_geoparquet(filsti) and filsti not in resultater[\"alle_filer\"]:\n",
    "                    resultater[\"allerede_geoparquet\"].append(filsti)\n",
    "                    resultater[\"alle_filer\"].append(filsti)\n",
    "    \n",
    "    # Oppsummering\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"OPPSUMMERING\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Katalog: {rotmappesti}\")\n",
    "    if mappestruktur:\n",
    "        print(f\"Mappestruktur: Raw={rå_mappesti}, Processed={prosessert_mappesti}\")\n",
    "    print(f\"Totalt antall geografiske filer funnet: {len(resultater['alle_filer'])}\")\n",
    "    print(f\"Antall parquet-filer som allerede var GeoParquet: {len(resultater['allerede_geoparquet'])}\")\n",
    "    print(f\"Antall filer konvertert til GeoParquet: {len(resultater['konvertert'])}\")\n",
    "    print(f\"Antall filer som ikke kunne konverteres: {len(resultater['feilet'])}\")\n",
    "    \n",
    "    if resultater[\"konvertert\"]:\n",
    "        print(\"\\nKonverterte filer:\")\n",
    "        for fil in resultater[\"konvertert\"]:\n",
    "            print(f\"- {fil}\")\n",
    "    \n",
    "    if resultater[\"feilet\"]:\n",
    "        print(\"\\nFiler som ikke kunne konverteres:\")\n",
    "        for fil in resultater[\"feilet\"]:\n",
    "            print(f\"- {fil}\")\n",
    "    \n",
    "    return resultater"
   ],
   "id": "7c63aac0595a3bd8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T12:14:58.375550Z",
     "start_time": "2025-03-07T12:14:58.364320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Angi rotmappen som skal behandles\n",
    "resultater = behandle_alle_filer_i_mappe(ROTMAPPE, opprett_struktur=True)"
   ],
   "id": "ad3ccb36d61dcd7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opprettet mappestruktur:\n",
      "- Raw: Data\\raw\n",
      "- Processed: Data\\processed\n",
      "\n",
      "==================================================\n",
      "OPPSUMMERING\n",
      "==================================================\n",
      "Katalog: Data\n",
      "Mappestruktur: Raw=Data\\raw, Processed=Data\\processed\n",
      "Totalt antall geografiske filer funnet: 0\n",
      "Antall parquet-filer som allerede var GeoParquet: 0\n",
      "Antall filer konvertert til GeoParquet: 0\n",
      "Antall filer som ikke kunne konverteres: 0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Filtering",
   "id": "a4732d2aef969375"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T12:21:27.919514Z",
     "start_time": "2025-03-07T12:21:27.738259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MÅ HENTE RIKTIG FIL!\n",
    "\n",
    "# Les geoparquet-filen\n",
    "fil_sti = f\"{ROTMAPPE}/raw/hais_2025-01-01.snappy.parquet\"\n",
    "gdf = gpd.read_parquet(fil_sti)\n",
    "\n",
    "# Filtrering basert på attributt/kolonne\n",
    "if 'date_time_utc' in gdf.columns:\n",
    "    filtrert_på_dato = gdf[gdf['date_time_utc'] >= '2025-01-01 12:00:00']\n",
    "    print(f\"Antall rader etter datofiltrering: {len(filtrert_på_dato)}\")\n",
    "    display(filtrert_på_dato.head())\n",
    "\n",
    "# Filtrering basert på geometri (f.eks. et område)\n",
    "område = box(5.0, 60.0, 11.0, 60.0)\n",
    "innenfor_område = gdf[gdf.geometry.intersects(område)]\n",
    "print(f\"Antall rader innenfor det definerte området: {len(innenfor_område)}\")\n",
    "display(innenfor_område.head())\n",
    "\n",
    "# Kombinert filtrering med flere kriterier\n",
    "if 'attributt' in gdf.columns:\n",
    "    kombinert_filter = gdf[(gdf.geometry.intersects(område)) & (gdf['attributt'] > 10)]\n",
    "    print(f\"Antall rader etter kombinert filtrering: {len(kombinert_filter)}\")\n",
    "    display(kombinert_filter.head())"
   ],
   "id": "a6e2988928bdc97e",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] Failed to open local file 'Data/raw/hais_2025-01-01.snappy.parquet'. Detail: [Windows error 2] Systemet finner ikke angitt fil.\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\geopandas\\io\\arrow.py:653\u001B[0m, in \u001B[0;36m_read_parquet_schema_and_metadata\u001B[1;34m(path, filesystem)\u001B[0m\n\u001B[0;32m    652\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 653\u001B[0m     schema \u001B[38;5;241m=\u001B[39m parquet\u001B[38;5;241m.\u001B[39mParquetDataset(path, filesystem\u001B[38;5;241m=\u001B[39mfilesystem, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\u001B[38;5;241m.\u001B[39mschema\n\u001B[0;32m    654\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1371\u001B[0m, in \u001B[0;36mParquetDataset.__init__\u001B[1;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001B[0m\n\u001B[0;32m   1368\u001B[0m     partitioning \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mHivePartitioning\u001B[38;5;241m.\u001B[39mdiscover(\n\u001B[0;32m   1369\u001B[0m         infer_dictionary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m-> 1371\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mdataset(path_or_paths, filesystem\u001B[38;5;241m=\u001B[39mfilesystem,\n\u001B[0;32m   1372\u001B[0m                            schema\u001B[38;5;241m=\u001B[39mschema, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mparquet_format,\n\u001B[0;32m   1373\u001B[0m                            partitioning\u001B[38;5;241m=\u001B[39mpartitioning,\n\u001B[0;32m   1374\u001B[0m                            ignore_prefixes\u001B[38;5;241m=\u001B[39mignore_prefixes)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\dataset.py:794\u001B[0m, in \u001B[0;36mdataset\u001B[1;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001B[0m\n\u001B[0;32m    793\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_path_like(source):\n\u001B[1;32m--> 794\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _filesystem_dataset(source, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    795\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(source, (\u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mlist\u001B[39m)):\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\dataset.py:476\u001B[0m, in \u001B[0;36m_filesystem_dataset\u001B[1;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001B[0m\n\u001B[0;32m    475\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 476\u001B[0m     fs, paths_or_selector \u001B[38;5;241m=\u001B[39m _ensure_single_source(source, filesystem)\n\u001B[0;32m    478\u001B[0m options \u001B[38;5;241m=\u001B[39m FileSystemFactoryOptions(\n\u001B[0;32m    479\u001B[0m     partitioning\u001B[38;5;241m=\u001B[39mpartitioning,\n\u001B[0;32m    480\u001B[0m     partition_base_dir\u001B[38;5;241m=\u001B[39mpartition_base_dir,\n\u001B[0;32m    481\u001B[0m     exclude_invalid_files\u001B[38;5;241m=\u001B[39mexclude_invalid_files,\n\u001B[0;32m    482\u001B[0m     selector_ignore_prefixes\u001B[38;5;241m=\u001B[39mselector_ignore_prefixes\n\u001B[0;32m    483\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\dataset.py:441\u001B[0m, in \u001B[0;36m_ensure_single_source\u001B[1;34m(path, filesystem)\u001B[0m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(path)\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m filesystem, paths_or_selector\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: Data/raw/hais_2025-01-01.snappy.parquet",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Les geoparquet-filen\u001B[39;00m\n\u001B[0;32m      2\u001B[0m fil_sti \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mROTMAPPE\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/raw/hais_2025-01-01.snappy.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 3\u001B[0m gdf \u001B[38;5;241m=\u001B[39m gpd\u001B[38;5;241m.\u001B[39mread_parquet(fil_sti)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Filtrering basert på attributt/kolonne\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time_utc\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m gdf\u001B[38;5;241m.\u001B[39mcolumns:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\geopandas\\io\\arrow.py:751\u001B[0m, in \u001B[0;36m_read_parquet\u001B[1;34m(path, columns, storage_options, bbox, **kwargs)\u001B[0m\n\u001B[0;32m    747\u001B[0m filesystem, path \u001B[38;5;241m=\u001B[39m _get_filesystem_path(\n\u001B[0;32m    748\u001B[0m     path, filesystem\u001B[38;5;241m=\u001B[39mfilesystem, storage_options\u001B[38;5;241m=\u001B[39mstorage_options\n\u001B[0;32m    749\u001B[0m )\n\u001B[0;32m    750\u001B[0m path \u001B[38;5;241m=\u001B[39m _expand_user(path)\n\u001B[1;32m--> 751\u001B[0m schema, metadata \u001B[38;5;241m=\u001B[39m _read_parquet_schema_and_metadata(path, filesystem)\n\u001B[0;32m    753\u001B[0m geo_metadata \u001B[38;5;241m=\u001B[39m _validate_and_decode_metadata(metadata)\n\u001B[0;32m    755\u001B[0m bbox_filter \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    756\u001B[0m     _get_parquet_bbox_filter(geo_metadata, bbox) \u001B[38;5;28;01mif\u001B[39;00m bbox \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    757\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\geopandas\\io\\arrow.py:655\u001B[0m, in \u001B[0;36m_read_parquet_schema_and_metadata\u001B[1;34m(path, filesystem)\u001B[0m\n\u001B[0;32m    653\u001B[0m     schema \u001B[38;5;241m=\u001B[39m parquet\u001B[38;5;241m.\u001B[39mParquetDataset(path, filesystem\u001B[38;5;241m=\u001B[39mfilesystem, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\u001B[38;5;241m.\u001B[39mschema\n\u001B[0;32m    654\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m--> 655\u001B[0m     schema \u001B[38;5;241m=\u001B[39m parquet\u001B[38;5;241m.\u001B[39mread_schema(path, filesystem\u001B[38;5;241m=\u001B[39mfilesystem)\n\u001B[0;32m    657\u001B[0m metadata \u001B[38;5;241m=\u001B[39m schema\u001B[38;5;241m.\u001B[39mmetadata\n\u001B[0;32m    659\u001B[0m \u001B[38;5;66;03m# read metadata separately to get the raw Parquet FileMetaData metadata\u001B[39;00m\n\u001B[0;32m    660\u001B[0m \u001B[38;5;66;03m# (pyarrow doesn't properly exposes those in schema.metadata for files\u001B[39;00m\n\u001B[0;32m    661\u001B[0m \u001B[38;5;66;03m# created by GDAL - https://issues.apache.org/jira/browse/ARROW-16688)\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2342\u001B[0m, in \u001B[0;36mread_schema\u001B[1;34m(where, memory_map, decryption_properties, filesystem)\u001B[0m\n\u001B[0;32m   2340\u001B[0m file_ctx \u001B[38;5;241m=\u001B[39m nullcontext()\n\u001B[0;32m   2341\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filesystem \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 2342\u001B[0m     file_ctx \u001B[38;5;241m=\u001B[39m where \u001B[38;5;241m=\u001B[39m filesystem\u001B[38;5;241m.\u001B[39mopen_input_file(where)\n\u001B[0;32m   2344\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m file_ctx:\n\u001B[0;32m   2345\u001B[0m     file \u001B[38;5;241m=\u001B[39m ParquetFile(\n\u001B[0;32m   2346\u001B[0m         where, memory_map\u001B[38;5;241m=\u001B[39mmemory_map,\n\u001B[0;32m   2347\u001B[0m         decryption_properties\u001B[38;5;241m=\u001B[39mdecryption_properties)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_fs.pyx:789\u001B[0m, in \u001B[0;36mpyarrow._fs.FileSystem.open_input_file\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001B[0m, in \u001B[0;36mpyarrow.lib.pyarrow_internal_check_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 2] Failed to open local file 'Data/raw/hais_2025-01-01.snappy.parquet'. Detail: [Windows error 2] Systemet finner ikke angitt fil.\r\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Partisjonerte geoparquet-filer",
   "id": "f99835eaa749eee4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# sett i funskjon?\n",
    "\n",
    "# 1. Opprett en mappe for partisjonerte data\n",
    "os.makedirs(PARTISJON_MAPPE_HOUR, exist_ok=True)\n",
    "\n",
    "# 2. Lag en kopi for å unngå SettingWithCopyWarning\n",
    "data_for_partisjonering = innenfor_område.copy()\n",
    "\n",
    "# 3. Bruk .loc for å legge til time-kolonnen\n",
    "data_for_partisjonering.loc[:, 'time'] = data_for_partisjonering['date_time_utc'].dt.hour\n",
    "\n",
    "# 4. Partisjonering basert på time og ship_type\n",
    "for time, gruppe_time in data_for_partisjonering.groupby('time'):\n",
    "    time_mappe = os.path.join(PARTISJON_MAPPE_HOUR, f\"time={time:02d}\")\n",
    "    os.makedirs(time_mappe, exist_ok=True)\n",
    "    \n",
    "    for ship_type, gruppe_final in gruppe_time.groupby('ship_type'):\n",
    "        skip_mappe = os.path.join(time_mappe, f\"ship_type={ship_type}\")\n",
    "        os.makedirs(skip_mappe, exist_ok=True)\n",
    "        \n",
    "        # Bruk en kopi av dataene\n",
    "        data_å_lagre = gruppe_final.copy()\n",
    "        \n",
    "        # Fjern partisjoneringskolonnen før lagring\n",
    "        fil_sti = os.path.join(skip_mappe, f\"data.parquet\")\n",
    "        data_å_lagre.drop('time', axis=1).to_parquet(fil_sti)\n",
    "        \n",
    "        print(f\"Skrevet {len(gruppe_final)} rader til {fil_sti}\")"
   ],
   "id": "476c31cf6ae8f343"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Dele opp i funskjoner\n",
    "\n",
    "# Hente data for en spesifikk måned\n",
    "ønsket_måned = \"2025-01\"\n",
    "måned_sti = os.path.join(PARTISJON_MAPPE_HOUR, f\"år_måned={ønsket_måned}\")\n",
    "\n",
    "if os.path.exists(måned_sti):\n",
    "    print(f\"Leser data for {ønsket_måned}...\")\n",
    "    # Finn alle geoparquet-filer i denne månedens mappe (inkludert undermapper)\n",
    "    filer = glob.glob(os.path.join(måned_sti, \"**/*.parquet\"), recursive=True)\n",
    "    # Les og kombiner alle filene\n",
    "    dataframes = []\n",
    "    for fil in filer:\n",
    "        gdf = gpd.read_parquet(fil)\n",
    "        dataframes.append(gdf)\n",
    "    \n",
    "    if dataframes:\n",
    "        månedsdata = pd.concat(dataframes)\n",
    "        print(f\"Hentet {len(månedsdata)} rader for {ønsket_måned}\")\n",
    "        display(månedsdata.head())\n",
    "    else:\n",
    "        print(f\"Ingen data funnet for {ønsket_måned}\")\n",
    "else:\n",
    "    print(f\"Ingen mappe funnet for {ønsket_måned}\")\n",
    "\n",
    "# Hente data for en spesifikk skipstype\n",
    "ønsket_skipstype = 70 \n",
    "skipstype_stier = glob.glob(os.path.join(PARTISJON_MAPPE_HOUR, f\"**/ship_type={ønsket_skipstype}/*.parquet\"), recursive=True)\n",
    "\n",
    "if skipstype_stier:\n",
    "    print(f\"\\nLeser data for skipstype {ønsket_skipstype}...\")\n",
    "    skipstype_dataframes = []\n",
    "    for fil in skipstype_stier:\n",
    "        gdf = gpd.read_parquet(fil)\n",
    "        skipstype_dataframes.append(gdf)\n",
    "    \n",
    "    skipstype_data = pd.concat(skipstype_dataframes)\n",
    "    print(f\"Hentet {len(skipstype_data)} rader for skipstype {ønsket_skipstype}\")\n",
    "    display(skipstype_data.head())\n",
    "else:\n",
    "    print(f\"Ingen data funnet for skipstype {ønsket_skipstype}\")"
   ],
   "id": "5fee57ee3dce0ca3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Opprett en mappe for partisjonerte data\n",
    "os.makedirs(PARTISJON_MAPPE_MIN, exist_ok=True)\n",
    "\n",
    "# 2. Lag en kopi for å unngå SettingWithCopyWarning\n",
    "data_for_partisjonering = innenfor_område.copy()\n",
    "\n",
    "# 3. Bruk .loc for å legge til 10-minutters intervall kolonne\n",
    "# Dette gir intervaller 0-143 for hele dagen (144 intervaller på 10 minutter)\n",
    "data_for_partisjonering.loc[:, 'minuttgruppe'] = (\n",
    "    data_for_partisjonering['date_time_utc'].dt.hour * 6 + \n",
    "    data_for_partisjonering['date_time_utc'].dt.minute // 10\n",
    ")\n",
    "\n",
    "# 4. Partisjonering basert på minuttgruppe og ship_type\n",
    "for minuttgruppe, gruppe_minutt in data_for_partisjonering.groupby('minuttgruppe'):\n",
    "    # Konverter minuttgruppe til time og minutt for mappe-strukturen\n",
    "    time = minuttgruppe // 6\n",
    "    minutt = (minuttgruppe % 6) * 10\n",
    "    \n",
    "    # Lag en lesbar mappestruktur (time_minutt=HH_MM)\n",
    "    minutt_mappe = os.path.join(PARTISJON_MAPPE_MIN, f\"time_minutt={time:02d}_{minutt:02d}\")\n",
    "    os.makedirs(minutt_mappe, exist_ok=True)\n",
    "    \n",
    "    for ship_type, gruppe_final in gruppe_minutt.groupby('ship_type'):\n",
    "        skip_mappe = os.path.join(minutt_mappe, f\"ship_type={ship_type}\")\n",
    "        os.makedirs(skip_mappe, exist_ok=True)\n",
    "        \n",
    "        # Bruk en kopi av dataene\n",
    "        data_å_lagre = gruppe_final.copy()\n",
    "        \n",
    "        # Fjern partisjoneringskolonnen før lagring\n",
    "        fil_sti = os.path.join(skip_mappe, f\"data.parquet\")\n",
    "        data_å_lagre.drop('minuttgruppe', axis=1).to_parquet(fil_sti)\n",
    "        \n",
    "        print(f\"Skrevet {len(gruppe_final)} rader til {fil_sti}\")"
   ],
   "id": "27d993f8fe7cea5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simulering av datastrømming",
   "id": "9c3d9653bf6fd8b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Simulert datastrømming med visualisering\n",
    "\n",
    "# Finn alle minutt-mapper sortert kronologisk\n",
    "alle_intervaller = sorted([d for d in os.listdir(PARTISJON_MAPPE_MIN) if d.startswith(\"time_minutt=\")])\n",
    "\n",
    "# Funksjon for å lage et kart med skipsbevegelser\n",
    "def vis_skip_på_kart(data, tidspunkt):\n",
    "    # Konverter til WGS84 koordinatsystem hvis nødvendig\n",
    "    if data.crs and data.crs != \"EPSG:4326\":\n",
    "        data = data.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Beregn senterpunkt for kartet\n",
    "    midpoint_lat = data.geometry.centroid.y.mean()\n",
    "    midpoint_lon = data.geometry.centroid.x.mean()\n",
    "    \n",
    "    # Opprett et kart sentrert på dataenes midtpunkt\n",
    "    m = folium.Map(location=[midpoint_lat, midpoint_lon], zoom_start=10)\n",
    "    \n",
    "    # Legg til en tittel\n",
    "    tittel_html = f'''\n",
    "    <h3 align=\"center\" style=\"font-size:16px\"><b>Skipsbevegelser: {tidspunkt}</b></h3>\n",
    "    '''\n",
    "    m.get_root().html.add_child(folium.Element(tittel_html))\n",
    "    \n",
    "    # Fargekoding basert på skipstype\n",
    "    ship_type_colors = {\n",
    "        30: 'blue',      # Fiskefartøy\n",
    "        31: 'green',     # Slepebåt\n",
    "        52: 'red',       # Passasjerskip\n",
    "        60: 'purple',    # Passasjerskip\n",
    "        70: 'orange',    # Lasteskip\n",
    "        80: 'darkblue',  # Tankskip\n",
    "        # Legg til flere skipstyper etter behov\n",
    "    }\n",
    "    \n",
    "    # Legg til hvert skip på kartet\n",
    "    for idx, row in data.iterrows():\n",
    "        # Bestem farge basert på skipstype\n",
    "        color = ship_type_colors.get(row['ship_type'], 'gray')\n",
    "        \n",
    "        # Hent koordinater\n",
    "        coords = (row.geometry.y, row.geometry.x)\n",
    "        \n",
    "        # Lag popup-info\n",
    "        popup_text = f\"\"\"\n",
    "        <b>Skip:</b> {row['ship_name']}<br>\n",
    "        <b>MMSI:</b> {row['mmsi']}<br>\n",
    "        <b>Type:</b> {row['ship_type']}<br>\n",
    "        <b>Hastighet:</b> {row['speed_over_ground']} knop<br>\n",
    "        <b>Kurs:</b> {row['course_over_ground']}°<br>\n",
    "        <b>Tidspunkt:</b> {row['date_time_utc']}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Legg til markør med retningspil\n",
    "        folium.Marker(\n",
    "            coords,\n",
    "            popup=folium.Popup(popup_text, max_width=300),\n",
    "            icon=folium.Icon(color=color, icon='ship', prefix='fa'),\n",
    "            tooltip=f\"{row['ship_name']} ({row['mmsi']})\"\n",
    "        ).add_to(m)\n",
    "        \n",
    "        # Tegn retningspil hvis kurs er tilgjengelig\n",
    "        if not pd.isna(row['course_over_ground']):\n",
    "            folium.RegularPolygonMarker(\n",
    "                coords,\n",
    "                fill_color=color,\n",
    "                number_of_sides=3,\n",
    "                radius=8,\n",
    "                rotation=row['course_over_ground'],\n",
    "                fill_opacity=0.6,\n",
    "                color='black',\n",
    "                weight=1\n",
    "            ).add_to(m)\n",
    "    \n",
    "    # Legg til tegnforklaring\n",
    "    legend_html = '''\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 50px; right: 50px; width: 150px; height: 160px; \n",
    "                border:2px solid grey; z-index:9999; font-size:12px;\n",
    "                background-color: white; padding: 10px;\n",
    "                border-radius: 5px;\">\n",
    "    <b>Skipstyper:</b><br>\n",
    "    '''\n",
    "    \n",
    "    for ship_type, color in ship_type_colors.items():\n",
    "        legend_html += f'<i class=\"fa fa-ship\" style=\"color:{color}\"></i> Type {ship_type}<br>'\n",
    "    \n",
    "    legend_html += '</div>'\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    \n",
    "    # Returner kartet\n",
    "    return m\n",
    "\n",
    "# Simuler datastrømming med visualisering\n",
    "print(\"\\nSimulerer datastrømming med visualisering...\")\n",
    "for intervall_dir in alle_intervaller:\n",
    "    # Hent time og minutt fra mappenavnet\n",
    "    match = re.search(r\"time_minutt=(\\d{2})_(\\d{2})\", intervall_dir)\n",
    "    if match:\n",
    "        time, minutt = match.groups()\n",
    "        tidspunkt = f\"{time}:{minutt}\"\n",
    "        \n",
    "        intervall_sti = os.path.join(PARTISJON_MAPPE_MIN, intervall_dir)\n",
    "        print(f\"\\nProsesserer data for {tidspunkt}...\")\n",
    "        \n",
    "        # Finn alle geoparquet-filer for dette intervallet\n",
    "        filer = glob.glob(os.path.join(intervall_sti, \"**/*.parquet\"), recursive=True)\n",
    "        \n",
    "        # Les og bearbeid hver fil\n",
    "        intervall_data = []\n",
    "        for fil in filer:\n",
    "            gdf = gpd.read_parquet(fil)\n",
    "            intervall_data.append(gdf)\n",
    "            \n",
    "        if intervall_data:\n",
    "            samlet_data = pd.concat(intervall_data)\n",
    "            print(f\"Kl. {tidspunkt}: Lastet {len(samlet_data)} rader fra {len(filer)} filer\")\n",
    "            \n",
    "            # Vis statistikker\n",
    "            print(f\"  Gjennomsnittlig hastighet (SOG): {samlet_data['speed_over_ground'].mean():.2f}\")\n",
    "            print(f\"  Antall unike skip: {samlet_data['mmsi'].nunique()}\")\n",
    "            \n",
    "            # Opprett og vis kart\n",
    "            kart = vis_skip_på_kart(samlet_data, f\"Kl. {tidspunkt}\")\n",
    "            \n",
    "            # Lagre kartet til en HTML-fil\n",
    "            # kart_filnavn = f\"kart_{time}_{minutt}.html\"\n",
    "            # kart.save(kart_filnavn)\n",
    "            # print(f\"  Kart lagret som {kart_filnavn}\")\n",
    "            \n",
    "            display(kart)\n",
    "            \n",
    "        else:\n",
    "            print(f\"Ingen data funnet for kl. {tidspunkt}\")\n",
    "        \n",
    "        # Simuler tid mellom intervaller\n",
    "        time_module.sleep(0.5)\n",
    "        \n",
    "print(\"\\nDatastrømming komplett!\")"
   ],
   "id": "3bc58bdf1671a9f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "572d26952da3eda5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
