{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Demo KartAI\n",
    "\n",
    "## Import pakker"
   ],
   "id": "611e68df1bc05d20"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:36.866078Z",
     "start_time": "2025-03-13T13:51:36.859141Z"
    }
   },
   "source": [
    "import os\n",
    "import fiona\n",
    "import pyarrow.parquet as pq\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import time as time_module\n",
    "import glob\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import builtins\n",
    "from shapely import wkt\n",
    "from shapely.geometry import box\n",
    "from IPython.display import display"
   ],
   "outputs": [],
   "execution_count": 382
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Konstante variabler",
   "id": "e4dbac65a8c79e7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:36.910438Z",
     "start_time": "2025-03-13T13:51:36.902095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ABSOLUTE_PATH = os.getcwd()\n",
    "ROT_FOLDER = \"Data\"\n",
    "ROT_PATH = os.path.join(ABSOLUTE_PATH, ROT_FOLDER)\n",
    "SUPPORTED_FORMATS = ['.parquet', '.geojson', '.json', '.shp', '.gpkg', '.csv', '.gml']\n",
    "FOLDER = {\n",
    "    'raw': os.path.join(ROT_PATH, 'raw'), \n",
    "    'processed': os.path.join(ROT_PATH, 'processed'), \n",
    "    'archive': os.path.join(ROT_PATH, 'archive'),\n",
    "    'logs': os.path.join(ROT_PATH, 'logs')\n",
    "}"
   ],
   "id": "c2c45b41c6ef7c1",
   "outputs": [],
   "execution_count": 383
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Diverse Sjekker",
   "id": "a24303bce050d61f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:36.935165Z",
     "start_time": "2025-03-13T13:51:36.927665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_geoparquet(file_path):\n",
    "    metadata = pq.read_metadata(file_path)\n",
    "    return \"geo\" in metadata.metadata if metadata.metadata else False"
   ],
   "id": "3f53b14e265ef7",
   "outputs": [],
   "execution_count": 384
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:36.963022Z",
     "start_time": "2025-03-13T13:51:36.956078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_target_file_path(source_file_path, target_folder):\n",
    "    \"\"\"Lager målfilsti basert på kildesti og evt. målmappe\"\"\"\n",
    "    filename = os.path.basename(source_file_path)\n",
    "    filename_Without_type = os.path.splitext(filename)[0]\n",
    "\n",
    "    if target_folder is None:\n",
    "        # Hvis ingen målmappe er angitt, bruk samme mappe med _geo.parquet\n",
    "        dir_navn = os.path.dirname(source_file_path)\n",
    "        return os.path.join(dir_navn, f\"{filename_Without_type}_geo.parquet\")\n",
    "    else:\n",
    "        # Hvis målmappe er angitt, plasser filen der\n",
    "        return os.path.join(source_file_path, f\"{filename_Without_type}.parquet\")"
   ],
   "id": "19718eff24acb2fb",
   "outputs": [],
   "execution_count": 385
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.004755Z",
     "start_time": "2025-03-13T13:51:36.997880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def valid_conversion(filepath, filename, filetype):\n",
    "    \"\"\"Sjekker om en fil er gyldig for konvertering\"\"\"\n",
    "    # Ikke konverter loggfiler\n",
    "    if filename == \"conversion_log\" or filepath.endswith(\"conversion_log.json\"):\n",
    "        return False\n",
    "\n",
    "    # Sjekk om filen har et støttet format\n",
    "    if filetype not in SUPPORTED_FORMATS:\n",
    "        return False\n",
    "\n",
    "    # Parquet-filer trenger ekstra sjekk\n",
    "    if filetype == '.parquet':\n",
    "        # Ikke ta med filer som allerede er GeoParquet\n",
    "        if is_geoparquet(filepath):\n",
    "            return False\n",
    "        # Ikke ta med filer som allerede har _geo suffix\n",
    "        if filename.endswith('_geo'):\n",
    "            return False\n",
    "\n",
    "    return True"
   ],
   "id": "d4062d5a73572bff",
   "outputs": [],
   "execution_count": 386
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.023643Z",
     "start_time": "2025-03-13T13:51:37.014764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_file_convert(filepath, filename, convert_files, convert_logs):\n",
    "    \"\"\"Sjekker om en fil trenger konvertering basert på konverteringshistorikk\"\"\"\n",
    "    # Sjekk loggfilen\n",
    "    if filename in convert_logs:\n",
    "        # Sjekk om den konverterte filen faktisk eksisterer\n",
    "        target_path = convert_logs[filename].get('målsti', None)\n",
    "        if target_path and os.path.exists(target_path):\n",
    "            # Filen finnes - sjekk om den har blitt endret siden sist konvertering\n",
    "            sist_endret_tid = os.path.getmtime(filepath)\n",
    "            sist_konvertert_tid = convert_logs[filename].get('tidspunkt', 0)\n",
    "            if sist_endret_tid <= sist_konvertert_tid:\n",
    "                return False\n",
    "        else:\n",
    "            # Filen i loggfilen eksisterer ikke - den bør konverteres på nytt\n",
    "            print(f\"Konvertert fil {target_path} finnes ikke, konverterer på nytt...\")\n",
    "            return True\n",
    "\n",
    "    # Sjekk om filen allerede finnes i processed-mappen\n",
    "    return filename not in convert_files"
   ],
   "id": "2f2448d60ecf9242",
   "outputs": [],
   "execution_count": 387
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get-funskjoner",
   "id": "d6be6864ec0fba8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.066539Z",
     "start_time": "2025-03-13T13:51:37.057686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_convert_files():\n",
    "    \"\"\"Henter ut alle allerede konverterte filer\"\"\"\n",
    "\n",
    "    # Return an empty set if the folder does not exist\n",
    "    if not os.path.exists(FOLDER['processed']):\n",
    "        return None\n",
    "\n",
    "    convert_files = set()\n",
    "    # Iterate through files in the directory\n",
    "    for root, _, filer in os.walk(FOLDER['processed']):\n",
    "        for fil in filer:\n",
    "            if fil.endswith('.parquet'):\n",
    "                base_navn = os.path.splitext(fil)[0]\n",
    "                convert_files.add(base_navn)\n",
    "\n",
    "    return convert_files"
   ],
   "id": "f17ec236bea9d31f",
   "outputs": [],
   "execution_count": 388
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.092922Z",
     "start_time": "2025-03-13T13:51:37.084596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_exists_geoparquet():\n",
    "    \"\"\"Finner eksisterende GeoParquet-filer\"\"\"\n",
    "    geoparquet_files = []\n",
    "\n",
    "    for root, _, files in os.walk(FOLDER['raw']):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                if is_geoparquet(filepath) and filepath not in files:\n",
    "                    geoparquet_files.append(filepath)\n",
    "\n",
    "    return geoparquet_files"
   ],
   "id": "6af345333fb92ebe",
   "outputs": [],
   "execution_count": 389
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.124466Z",
     "start_time": "2025-03-13T13:51:37.115455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_new_files():\n",
    "    \"\"\"Finner filer som ikke har blitt konvertert ennå\"\"\"\n",
    "    convert_files = get_convert_files()\n",
    "    #convert_logs = read_log()\n",
    "\n",
    "    nye_filer = []\n",
    "\n",
    "    for root, _, files in os.walk(FOLDER['raw']):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            filetype = os.path.splitext(file)[1].lower()\n",
    "            filename = os.path.splitext(file)[0]\n",
    "\n",
    "            # Sjekk om filen er gyldig for konvertering\n",
    "            if not valid_conversion(filepath, filename, filetype):\n",
    "                continue\n",
    "\n",
    "            # Sjekk om filen trenger konvertering\n",
    "            #if check_file_convert(filepath, filename, convert_files):\n",
    "            nye_filer.append(filepath)\n",
    "\n",
    "    return nye_filer"
   ],
   "id": "45ddad71e12007b1",
   "outputs": [],
   "execution_count": 390
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Konverting til parquet\n",
    "\n",
    "#### sjekk om det er Parquet"
   ],
   "id": "ae522128d05d8443"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.162613Z",
     "start_time": "2025-03-13T13:51:37.155457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_latlon_to_gdf(df):\n",
    "    \"\"\"Convert a DataFrame with latitude/longitude columns to a GeoDataFrame.\"\"\"\n",
    "    if {'longitude', 'latitude'}.issubset(df.columns):\n",
    "            df['geometry'] = gpd.points_from_xy(df['longitude'], df['latitude'])\n",
    "            return gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=\"EPSG:4326\")\n",
    "    return None"
   ],
   "id": "5c9d81a5786fefa3",
   "outputs": [],
   "execution_count": 391
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.196025Z",
     "start_time": "2025-03-13T13:51:37.188143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_wkt_geometry(df):\n",
    "    \"\"\"Detect a potential geometry column in WKT format and convert it to a GeoDataFrame.\"\"\"\n",
    "    pattern = re.compile(r'geom|coord|point|polygon|linestring|wkt', re.IGNORECASE)\n",
    "    geo_col = [col for col in df.columns if pattern.search(col)]\n",
    "\n",
    "    for col in geo_col:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Prøv å konvertere fra WKT-format\n",
    "            geom = df[col].apply(wkt.loads)\n",
    "            return gpd.GeoDataFrame(df, geometry=geom, crs=\"EPSG:4326\")\n",
    "    return None"
   ],
   "id": "de8b1f52c45d7095",
   "outputs": [],
   "execution_count": 392
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.228208Z",
     "start_time": "2025-03-13T13:51:37.220115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def converter_parquet(path):\n",
    "    \"\"\"Konverterer parquet-fil til GeoDataFrame\"\"\"\n",
    "    if isinstance(path, str) and os.path.exists(path):\n",
    "        df = pd.read_parquet(path)\n",
    "    \n",
    "    # Try latitude/longitude conversion first\n",
    "    gdf = convert_latlon_to_gdf(df)\n",
    "    if gdf is not None:\n",
    "        return gdf\n",
    "\n",
    "    # Try detecting and converting WKT geometry\n",
    "    gdf = detect_wkt_geometry(df)\n",
    "    if gdf is not None:\n",
    "        return gdf\n",
    "\n",
    "    print(\"No valid geometry columns found.\")\n",
    "    return None\n",
    "    "
   ],
   "id": "a865c8920c0ff04d",
   "outputs": [],
   "execution_count": 393
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Sjekk om det er CSV",
   "id": "737b8d5693a3e928"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.259794Z",
     "start_time": "2025-03-13T13:51:37.250793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def converter_csv(filsti):\n",
    "    \"\"\"Konverterer CSV-fil til GeoDataFrame\"\"\"\n",
    "    df = pd.read_csv(filsti)\n",
    "\n",
    "    # Sjekk for vanlige lat/long kolonnenavn\n",
    "    lat_kolonner = ['latitude', 'lat', 'y', 'breddegrad']\n",
    "    lon_kolonner = ['longitude', 'long', 'lon', 'x', 'lengdegrad']\n",
    "\n",
    "    lat_col = next((col for col in lat_kolonner if col in df.columns), None)\n",
    "    lon_col = next((col for col in lon_kolonner if col in df.columns), None)\n",
    "\n",
    "    if lat_col and lon_col:\n",
    "        return gpd.GeoDataFrame(\n",
    "            df,\n",
    "            geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "\n",
    "    return None"
   ],
   "id": "e4a03deea3827d8d",
   "outputs": [],
   "execution_count": 394
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Sjekk om det er gml",
   "id": "79f3bb0bb1a051e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.285910Z",
     "start_time": "2025-03-13T13:51:37.278627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_gml(path):\n",
    "    layers = fiona.listlayers(path)\n",
    "    if layers:\n",
    "        print(f\"GML-fil har {len(layers)} lag: {', '.join(layers)}\")\n",
    "        # Les det første laget, som er standard\n",
    "        return gpd.read_file(path, layer=layers[0])\n",
    "    else:\n",
    "        return gpd.read_file(path)"
   ],
   "id": "b077ea6b61b894ab",
   "outputs": [],
   "execution_count": 395
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Konvertere filer til geoparquet",
   "id": "db52e2648e3826b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.313497Z",
     "start_time": "2025-03-13T13:51:37.304196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_filetype(path):\n",
    "    \"\"\"Konverterer fil til GeoDataFrame basert på filendelse\"\"\"\n",
    "    \n",
    "    filetype = os.path.splitext(path)[1].lower()\n",
    "    if filetype == '.parquet':\n",
    "        return converter_parquet(path)\n",
    "\n",
    "    elif filetype in ['.geojson', '.json', '.shp', '.gpkg']:\n",
    "        return gpd.read_file(path)\n",
    "\n",
    "    elif filetype == '.gml':\n",
    "        return convert_gml(path)\n",
    "    \n",
    "    elif filetype == '.csv':\n",
    "        return converter_csv(path)\n",
    "\n",
    "    print(f\"Klarte ikke å finne match konverting til filendelse {filetype}\")\n",
    "    return None"
   ],
   "id": "93187f73daa1918a",
   "outputs": [],
   "execution_count": 396
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.338188Z",
     "start_time": "2025-03-13T13:51:37.331341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def converter_to_geoparquet(source_filepath, target_folder_path=None):\n",
    "    \"\"\"Konverterer ulike geografiske formater til GeoParquet-format\"\"\"\n",
    "    \n",
    "    target_file_path = create_target_file_path(source_filepath, target_folder_path)\n",
    "    \n",
    "    # Sørg for at målmappen eksisterer\n",
    "    os.makedirs(os.path.dirname(source_filepath), exist_ok=True)\n",
    "    \n",
    "    # Konverter filen basert på filtype\n",
    "    gdf = convert_filetype(source_filepath)\n",
    "\n",
    "    # Hvis vi har en gyldig geodataframe, lagre som GeoParquet\n",
    "    if gdf is not None:\n",
    "        gdf.to_parquet(target_file_path)\n",
    "        return target_file_path\n",
    "\n",
    "    return False"
   ],
   "id": "30f58dc5a1f4c748",
   "outputs": [],
   "execution_count": 397
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Logging",
   "id": "35b5627148fb94aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.363003Z",
     "start_time": "2025-03-13T13:51:37.356732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_log():\n",
    "    \"\"\"Leser inn eksisterende konverteringslogg\"\"\"\n",
    "    if not os.path.exists(FOLDER['logs']):\n",
    "        return {}\n",
    "    with builtins.open(FOLDER['logs'], 'r') as f:\n",
    "        return json.load(f)"
   ],
   "id": "3c1d24a9b8638a5b",
   "outputs": [],
   "execution_count": 398
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.386860Z",
     "start_time": "2025-03-13T13:51:37.379525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_logs(file, target_file_path):\n",
    "    \"\"\"Lager et nytt logginnslag for en konvertert fil\"\"\"\n",
    "    base_name = os.path.splitext(os.path.basename(fil))[0]\n",
    "    today = datetime.datetime.now()\n",
    "\n",
    "    return {\n",
    "        base_name: {\n",
    "            'kildesti': file,\n",
    "            'målsti': target_file_path,\n",
    "            'tidspunkt': today.timestamp(),\n",
    "            'dato': today.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "    }"
   ],
   "id": "2465904fa5f70a5f",
   "outputs": [],
   "execution_count": 399
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.409756Z",
     "start_time": "2025-03-13T13:51:37.402756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_log(logdata):\n",
    "    \"\"\"Lagrer konverteringslogg til fil\"\"\"\n",
    "    with builtins.open(FOLDER['logs'], 'w') as f:\n",
    "        json.dump(logdata, f, indent=2)"
   ],
   "id": "b3509d226724a3c2",
   "outputs": [],
   "execution_count": 400
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.433660Z",
     "start_time": "2025-03-13T13:51:37.426659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_logs(file, target_file_path):\n",
    "    \"\"\"Oppdaterer konverteringsloggen med ny filinfo\"\"\"\n",
    "    # Last inn eksisterende logg\n",
    "    convert_logger = read_log()\n",
    "\n",
    "    # Lag og legg til nytt logginnslag\n",
    "    new_log = generate_logs(file, target_file_path)\n",
    "    convert_logger.update(new_log)\n",
    "\n",
    "    # Lagre oppdatert logg\n",
    "    save_log(convert_logger)"
   ],
   "id": "fe5102b58c6498bf",
   "outputs": [],
   "execution_count": 401
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup",
   "id": "5301d4a09d677744"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.459571Z",
     "start_time": "2025-03-13T13:51:37.450641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def setup():\n",
    "    # Sjekk om mappen finnes, hvis ikke - lag den\n",
    "    if not os.path.exists(ROT_FOLDER):\n",
    "        os.makedirs(ROT_FOLDER, exist_ok=True)\n",
    "        print(f\"Opprettet rotmappe: {ROT_FOLDER}\")\n",
    "    \n",
    "        # Opprett undermappe-struktur\n",
    "        print(f\"Creating folder in {ROT_FOLDER}\")\n",
    "        for folderName in FOLDER:\n",
    "            os.makedirs(FOLDER[folderName], exist_ok=True)\n",
    "            print(f\"Opprettet mappe: {FOLDER[folderName]}\")\n",
    "    else:\n",
    "        print(f\"Mappen {ROT_FOLDER} finnes allerede\")"
   ],
   "id": "3eabcf597ae29e6c",
   "outputs": [],
   "execution_count": 402
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hovedfunskjoner",
   "id": "ccddfbd1859b7cb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.484723Z",
     "start_time": "2025-03-13T13:51:37.476721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def konverter_filer(new_files):\n",
    "    \"\"\"Konverterer filer og oppdaterer logg\"\"\"\n",
    "    resultater = {\n",
    "        \"konvertert\": [],\n",
    "        \"feilet\": []\n",
    "    }\n",
    "\n",
    "    for filepath in new_files:\n",
    "        # Konverter filen\n",
    "        target_filepath = converter_parquet(filepath)\n",
    "\n",
    "        if target_filepath:\n",
    "            resultater[\"konvertert\"].append(filepath)\n",
    "            update_logs(FOLDER['log'], filepath)\n",
    "        else:\n",
    "            resultater[\"feilet\"].append(filepath)\n",
    "\n",
    "    return resultater"
   ],
   "id": "f8e36a76b73e1e12",
   "outputs": [],
   "execution_count": 403
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.513058Z",
     "start_time": "2025-03-13T13:51:37.503629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def skriv_oppsummering(resultater):\n",
    "    \"\"\"Skriver oppsummering av konverteringsprosessen\"\"\"\n",
    "    print(\"\\n\")\n",
    "    print(\"OPPSUMMERING\")\n",
    "    print(f\"Katalog: {ROT_PATH}\")\n",
    "    print(f\"Mappestruktur: Raw={FOLDER['raw']}, Processed={FOLDER['processed']}\")\n",
    "\n",
    "    totalt_antall = len(resultater[\"alle_filer\"])\n",
    "    antall_allerede_geo = len(resultater[\"allerede_geoparquet\"])\n",
    "    antall_konvertert = len(resultater[\"konvertert\"])\n",
    "    antall_feilet = len(resultater[\"feilet\"])\n",
    "\n",
    "    print(f\"Totalt antall geografiske filer funnet: {totalt_antall}\")\n",
    "    print(f\"Antall parquet-filer som allerede var GeoParquet: {antall_allerede_geo}\")\n",
    "    print(f\"Antall filer konvertert til GeoParquet: {antall_konvertert}\")\n",
    "    print(f\"Antall filer som ikke kunne konverteres: {antall_feilet}\")\n",
    "\n",
    "    if resultater[\"konvertert\"]:\n",
    "        print(\"\\nKonverterte filer:\")\n",
    "        for fil in resultater[\"konvertert\"]:\n",
    "            print(f\"- {fil}\")\n",
    "\n",
    "    if resultater[\"feilet\"]:\n",
    "        print(\"\\nFiler som ikke kunne konverteres:\")\n",
    "        for fil in resultater[\"feilet\"]:\n",
    "            print(f\"- {fil}\")"
   ],
   "id": "ac0850e580dbc2ae",
   "outputs": [],
   "execution_count": 404
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:37.542849Z",
     "start_time": "2025-03-13T13:51:37.531367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def behandle_alle_filer_i_mappe():\n",
    "    \"\"\"Behandler alle geografiske filer i angitt mappe og konverterer til GeoParquet.\"\"\"\n",
    "    # Validering og mappestruktur (gjør dette først!)\n",
    "    if not os.path.exists(ROT_PATH):\n",
    "        print(f\"Vennligst oppgi en gyldig mappe som du har tilgang til.\")\n",
    "        print(f\"For eksempel: './data' (relativ sti) eller '~/data' (i hjemmekatalogen)\")\n",
    "        return None\n",
    "\n",
    "    # Uansett om opprett_struktur er True eller False, sørg for at processed-mappen eksisterer\n",
    "    processed_sti = FOLDER['processed']\n",
    "    if not os.path.exists(processed_sti):\n",
    "        try:\n",
    "            os.makedirs(processed_sti, exist_ok=True)\n",
    "            print(f\"Opprettet processed-mappe: {processed_sti}\")\n",
    "        except Exception as e:\n",
    "            print(f\"FEIL: Kunne ikke opprette processed-mappen: {e}\")\n",
    "\n",
    "    # Bruk processed-sti som målmappe i stedet for prosessert_mappesti fra oppsett_mappestier\n",
    "    if not os.path.exists(processed_sti):\n",
    "        try:\n",
    "            os.makedirs(processed_sti, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"FEIL: Kunne ikke opprette målmappen '{processed_sti}': {e}\")\n",
    "            return None\n",
    "\n",
    "    # Finn nye filer som trenger konvertering\n",
    "    nye_filer = get_new_files()\n",
    "\n",
    "    # Konverter filene og samle resultatene - bruk processed_sti som målmappe\n",
    "    konverteringsresultater = konverter_filer(\n",
    "        nye_filer\n",
    "    )\n",
    "\n",
    "    # Finn eksisterende GeoParquet-filer\n",
    "    allerede_geoparquet = get_exists_geoparquet()\n",
    "\n",
    "    # Sammenstill resultater\n",
    "    resultater = {\n",
    "        \"alle_filer\": nye_filer,\n",
    "        \"konvertert\": konverteringsresultater[\"konvertert\"],\n",
    "        \"feilet\": konverteringsresultater[\"feilet\"],\n",
    "        \"allerede_geoparquet\": allerede_geoparquet\n",
    "    }\n",
    "    skriv_oppsummering(resultater)\n",
    "\n",
    "    return resultater"
   ],
   "id": "2cc926862d6b585f",
   "outputs": [],
   "execution_count": 405
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:39.021548Z",
     "start_time": "2025-03-13T13:51:37.560214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sjekk at mappen eksisterer før vi fortsetter\n",
    "setup()\n",
    "\n",
    "# Kjører konverteringsprosessen\n",
    "resultater = behandle_alle_filer_i_mappe()\n",
    "\n",
    "# Viser resultatet\n",
    "if resultater:\n",
    "    print(f\"\\nTotalt konverterte filer: {len(resultater['konvertert'])}\")\n",
    "    print(f\"Filer som allerede var i GeoParquet-format: {len(resultater['allerede_geoparquet'])}\")\n",
    "else:\n",
    "    print(\"Konverteringen feilet. Sjekk filtype og format!\")"
   ],
   "id": "15f78855fed2bda2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mappen Data finnes allerede\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a GeoDataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_57800\\3199092158.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Sjekk at mappen eksisterer før vi fortsetter\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0msetup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;31m# Kjører konverteringsprosessen\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mresultater\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbehandle_alle_filer_i_mappe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;31m# Viser resultatet\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mresultater\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_57800\\2098097654.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[1;31m# Finn nye filer som trenger konvertering\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[0mnye_filer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_new_files\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[1;31m# Konverter filene og samle resultatene - bruk processed_sti som målmappe\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 30\u001B[1;33m     konverteringsresultater = konverter_filer(\n\u001B[0m\u001B[0;32m     31\u001B[0m         \u001B[0mnye_filer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m     \u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_57800\\3058559335.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(new_files)\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mfilepath\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mnew_files\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m         \u001B[1;31m# Konverter filen\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[0mtarget_filepath\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconverter_parquet\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[0mtarget_filepath\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m             \u001B[0mresultater\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"konvertert\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m             \u001B[0mupdate_logs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mFOLDER\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'log'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilepath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1575\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mfinal\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1576\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__nonzero__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mNoReturn\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1577\u001B[1;33m         raise ValueError(\n\u001B[0m\u001B[0;32m   1578\u001B[0m             \u001B[1;33mf\"\u001B[0m\u001B[1;33mThe truth value of a \u001B[0m\u001B[1;33m{\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m is ambiguous. \u001B[0m\u001B[1;33m\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1579\u001B[0m             \u001B[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1580\u001B[0m         \u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: The truth value of a GeoDataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "execution_count": 406
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filtrering",
   "id": "6939ebaa962772bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:39.094733700Z",
     "start_time": "2025-03-13T13:40:26.498348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Les geoparquet-filen\n",
    "fil_sti = \"data/raw/hais_2025-01-01.snappy.parquet\"\n",
    "gdf = gpd.read_parquet(fil_sti)\n",
    "\n",
    "# Filtrering basert på attributt/kolonne\n",
    "if 'date_time_utc' in gdf.columns:\n",
    "    filtrert_på_dato = gdf[gdf['date_time_utc'] >= '2025-01-01 16:00:00']\n",
    "    print(f\"Antall rader etter datofiltrering: {len(filtrert_på_dato)}\")\n",
    "    display(filtrert_på_dato.head())\n",
    "\n",
    "# Filtrering basert på geometri (f.eks. et område)\n",
    "område = box(5.0, 60.0, 11.0, 60.0)\n",
    "innenfor_område = gdf[gdf.geometry.intersects(område)]\n",
    "print(f\"Antall rader innenfor det definerte området: {len(innenfor_område)}\")\n",
    "display(innenfor_område.head())\n",
    "\n",
    "# Kombinert filtrering med flere kriterier\n",
    "if 'attributt' in gdf.columns:\n",
    "    kombinert_filter = gdf[(gdf.geometry.intersects(område)) & (gdf['attributt'] > 10)]\n",
    "    print(f\"Antall rader etter kombinert filtrering: {len(kombinert_filter)}\")\n",
    "    display(kombinert_filter.head())"
   ],
   "id": "cb4e32bd4b79ae46",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] Failed to open local file 'data/raw/hais_2025-01-01.snappy.parquet'. Detail: [Windows error 2] Systemet finner ikke angitt fil.\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\geopandas\\io\\arrow.py:653\u001B[0m, in \u001B[0;36m_read_parquet_schema_and_metadata\u001B[1;34m(path, filesystem)\u001B[0m\n\u001B[0;32m    652\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 653\u001B[0m     schema \u001B[38;5;241m=\u001B[39m parquet\u001B[38;5;241m.\u001B[39mParquetDataset(path, filesystem\u001B[38;5;241m=\u001B[39mfilesystem, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\u001B[38;5;241m.\u001B[39mschema\n\u001B[0;32m    654\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1371\u001B[0m, in \u001B[0;36mParquetDataset.__init__\u001B[1;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001B[0m\n\u001B[0;32m   1368\u001B[0m     partitioning \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mHivePartitioning\u001B[38;5;241m.\u001B[39mdiscover(\n\u001B[0;32m   1369\u001B[0m         infer_dictionary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m-> 1371\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mdataset(path_or_paths, filesystem\u001B[38;5;241m=\u001B[39mfilesystem,\n\u001B[0;32m   1372\u001B[0m                            schema\u001B[38;5;241m=\u001B[39mschema, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mparquet_format,\n\u001B[0;32m   1373\u001B[0m                            partitioning\u001B[38;5;241m=\u001B[39mpartitioning,\n\u001B[0;32m   1374\u001B[0m                            ignore_prefixes\u001B[38;5;241m=\u001B[39mignore_prefixes)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\dataset.py:794\u001B[0m, in \u001B[0;36mdataset\u001B[1;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001B[0m\n\u001B[0;32m    793\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_path_like(source):\n\u001B[1;32m--> 794\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _filesystem_dataset(source, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    795\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(source, (\u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mlist\u001B[39m)):\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\dataset.py:476\u001B[0m, in \u001B[0;36m_filesystem_dataset\u001B[1;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001B[0m\n\u001B[0;32m    475\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 476\u001B[0m     fs, paths_or_selector \u001B[38;5;241m=\u001B[39m _ensure_single_source(source, filesystem)\n\u001B[0;32m    478\u001B[0m options \u001B[38;5;241m=\u001B[39m FileSystemFactoryOptions(\n\u001B[0;32m    479\u001B[0m     partitioning\u001B[38;5;241m=\u001B[39mpartitioning,\n\u001B[0;32m    480\u001B[0m     partition_base_dir\u001B[38;5;241m=\u001B[39mpartition_base_dir,\n\u001B[0;32m    481\u001B[0m     exclude_invalid_files\u001B[38;5;241m=\u001B[39mexclude_invalid_files,\n\u001B[0;32m    482\u001B[0m     selector_ignore_prefixes\u001B[38;5;241m=\u001B[39mselector_ignore_prefixes\n\u001B[0;32m    483\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\dataset.py:441\u001B[0m, in \u001B[0;36m_ensure_single_source\u001B[1;34m(path, filesystem)\u001B[0m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(path)\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m filesystem, paths_or_selector\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: data/raw/hais_2025-01-01.snappy.parquet",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[331], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Les geoparquet-filen\u001B[39;00m\n\u001B[0;32m      2\u001B[0m fil_sti \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/raw/hais_2025-01-01.snappy.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 3\u001B[0m gdf \u001B[38;5;241m=\u001B[39m gpd\u001B[38;5;241m.\u001B[39mread_parquet(fil_sti)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Filtrering basert på attributt/kolonne\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time_utc\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m gdf\u001B[38;5;241m.\u001B[39mcolumns:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\geopandas\\io\\arrow.py:751\u001B[0m, in \u001B[0;36m_read_parquet\u001B[1;34m(path, columns, storage_options, bbox, **kwargs)\u001B[0m\n\u001B[0;32m    747\u001B[0m filesystem, path \u001B[38;5;241m=\u001B[39m _get_filesystem_path(\n\u001B[0;32m    748\u001B[0m     path, filesystem\u001B[38;5;241m=\u001B[39mfilesystem, storage_options\u001B[38;5;241m=\u001B[39mstorage_options\n\u001B[0;32m    749\u001B[0m )\n\u001B[0;32m    750\u001B[0m path \u001B[38;5;241m=\u001B[39m _expand_user(path)\n\u001B[1;32m--> 751\u001B[0m schema, metadata \u001B[38;5;241m=\u001B[39m _read_parquet_schema_and_metadata(path, filesystem)\n\u001B[0;32m    753\u001B[0m geo_metadata \u001B[38;5;241m=\u001B[39m _validate_and_decode_metadata(metadata)\n\u001B[0;32m    755\u001B[0m bbox_filter \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    756\u001B[0m     _get_parquet_bbox_filter(geo_metadata, bbox) \u001B[38;5;28;01mif\u001B[39;00m bbox \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    757\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\geopandas\\io\\arrow.py:655\u001B[0m, in \u001B[0;36m_read_parquet_schema_and_metadata\u001B[1;34m(path, filesystem)\u001B[0m\n\u001B[0;32m    653\u001B[0m     schema \u001B[38;5;241m=\u001B[39m parquet\u001B[38;5;241m.\u001B[39mParquetDataset(path, filesystem\u001B[38;5;241m=\u001B[39mfilesystem, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\u001B[38;5;241m.\u001B[39mschema\n\u001B[0;32m    654\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m--> 655\u001B[0m     schema \u001B[38;5;241m=\u001B[39m parquet\u001B[38;5;241m.\u001B[39mread_schema(path, filesystem\u001B[38;5;241m=\u001B[39mfilesystem)\n\u001B[0;32m    657\u001B[0m metadata \u001B[38;5;241m=\u001B[39m schema\u001B[38;5;241m.\u001B[39mmetadata\n\u001B[0;32m    659\u001B[0m \u001B[38;5;66;03m# read metadata separately to get the raw Parquet FileMetaData metadata\u001B[39;00m\n\u001B[0;32m    660\u001B[0m \u001B[38;5;66;03m# (pyarrow doesn't properly exposes those in schema.metadata for files\u001B[39;00m\n\u001B[0;32m    661\u001B[0m \u001B[38;5;66;03m# created by GDAL - https://issues.apache.org/jira/browse/ARROW-16688)\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2342\u001B[0m, in \u001B[0;36mread_schema\u001B[1;34m(where, memory_map, decryption_properties, filesystem)\u001B[0m\n\u001B[0;32m   2340\u001B[0m file_ctx \u001B[38;5;241m=\u001B[39m nullcontext()\n\u001B[0;32m   2341\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filesystem \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 2342\u001B[0m     file_ctx \u001B[38;5;241m=\u001B[39m where \u001B[38;5;241m=\u001B[39m filesystem\u001B[38;5;241m.\u001B[39mopen_input_file(where)\n\u001B[0;32m   2344\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m file_ctx:\n\u001B[0;32m   2345\u001B[0m     file \u001B[38;5;241m=\u001B[39m ParquetFile(\n\u001B[0;32m   2346\u001B[0m         where, memory_map\u001B[38;5;241m=\u001B[39mmemory_map,\n\u001B[0;32m   2347\u001B[0m         decryption_properties\u001B[38;5;241m=\u001B[39mdecryption_properties)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_fs.pyx:789\u001B[0m, in \u001B[0;36mpyarrow._fs.FileSystem.open_input_file\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001B[0m, in \u001B[0;36mpyarrow.lib.pyarrow_internal_check_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 2] Failed to open local file 'data/raw/hais_2025-01-01.snappy.parquet'. Detail: [Windows error 2] Systemet finner ikke angitt fil.\r\n"
     ]
    }
   ],
   "execution_count": 331
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Partisjonerte",
   "id": "35ebee5c9f161f2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:39.095732900Z",
     "start_time": "2025-03-13T09:31:53.222992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Opprett en mappe for partisjonerte data\n",
    "output_mappe = \"Data/partitioned_hour\"\n",
    "os.makedirs(output_mappe, exist_ok=True)\n",
    "\n",
    "# 2. Lag en kopi for å unngå SettingWithCopyWarning\n",
    "data_for_partisjonering = innenfor_område.copy()\n",
    "\n",
    "# 3. Bruk .loc for å legge til time-kolonnen\n",
    "data_for_partisjonering.loc[:, 'time'] = data_for_partisjonering['date_time_utc'].dt.hour\n",
    "\n",
    "# 4. Partisjonering basert på time og ship_type\n",
    "for time, gruppe_time in data_for_partisjonering.groupby('time'):\n",
    "    time_mappe = os.path.join(output_mappe, f\"time={time:02d}\")\n",
    "    os.makedirs(time_mappe, exist_ok=True)\n",
    "    \n",
    "    for ship_type, gruppe_final in gruppe_time.groupby('ship_type'):\n",
    "        skip_mappe = os.path.join(time_mappe, f\"ship_type={ship_type}\")\n",
    "        os.makedirs(skip_mappe, exist_ok=True)\n",
    "        \n",
    "        # Bruk en kopi av dataene\n",
    "        data_å_lagre = gruppe_final.copy()\n",
    "        \n",
    "        # Fjern partisjoneringskolonnen før lagring\n",
    "        fil_sti = os.path.join(skip_mappe, f\"data.parquet\")\n",
    "        data_å_lagre.drop('time', axis=1).to_parquet(fil_sti)\n",
    "        \n",
    "        print(f\"Skrevet {len(gruppe_final)} rader til {fil_sti}\")"
   ],
   "id": "56f4f3ed40e778b5",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'innenfor_område' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(output_mappe, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# 2. Lag en kopi for å unngå SettingWithCopyWarning\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m data_for_partisjonering \u001B[38;5;241m=\u001B[39m innenfor_område\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# 3. Bruk .loc for å legge til time-kolonnen\u001B[39;00m\n\u001B[0;32m     12\u001B[0m data_for_partisjonering\u001B[38;5;241m.\u001B[39mloc[:, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data_for_partisjonering[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time_utc\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mhour\n",
      "\u001B[1;31mNameError\u001B[0m: name 'innenfor_område' is not defined"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:39.095732900Z",
     "start_time": "2025-03-13T09:31:56.293340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "partisjon_mappe = \"Data/partitioned_hour\"\n",
    "\n",
    "# Hente data for en spesifikk måned\n",
    "ønsket_måned = \"2025-01\"\n",
    "måned_sti = os.path.join(partisjon_mappe, f\"år_måned={ønsket_måned}\")\n",
    "\n",
    "if os.path.exists(måned_sti):\n",
    "    print(f\"Leser data for {ønsket_måned}...\")\n",
    "    # Finn alle geoparquet-filer i denne månedens mappe (inkludert undermapper)\n",
    "    filer = glob.glob(os.path.join(måned_sti, \"**/*.parquet\"), recursive=True)\n",
    "    # Les og kombiner alle filene\n",
    "    dataframes = []\n",
    "    for fil in filer:\n",
    "        gdf = gpd.read_parquet(fil)\n",
    "        dataframes.append(gdf)\n",
    "    \n",
    "    if dataframes:\n",
    "        månedsdata = pd.concat(dataframes)\n",
    "        print(f\"Hentet {len(månedsdata)} rader for {ønsket_måned}\")\n",
    "        display(månedsdata.head())\n",
    "    else:\n",
    "        print(f\"Ingen data funnet for {ønsket_måned}\")\n",
    "else:\n",
    "    print(f\"Ingen mappe funnet for {ønsket_måned}\")\n",
    "\n",
    "# Hente data for en spesifikk skipstype\n",
    "ønsket_skipstype = 30\n",
    "skipstype_stier = glob.glob(os.path.join(partisjon_mappe, f\"**/ship_type={ønsket_skipstype}/*.parquet\"), recursive=True)\n",
    "\n",
    "if skipstype_stier:\n",
    "    print(f\"\\nLeser data for skipstype {ønsket_skipstype}...\")\n",
    "    skipstype_dataframes = []\n",
    "    for fil in skipstype_stier:\n",
    "        gdf = gpd.read_parquet(fil)\n",
    "        skipstype_dataframes.append(gdf)\n",
    "    \n",
    "    skipstype_data = pd.concat(skipstype_dataframes)\n",
    "    print(f\"Hentet {len(skipstype_data)} rader for skipstype {ønsket_skipstype}\")\n",
    "    display(skipstype_data.head())\n",
    "else:\n",
    "    print(f\"Ingen data funnet for skipstype {ønsket_skipstype}\")"
   ],
   "id": "f6a458ea92d74d54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingen mappe funnet for 2025-01\n",
      "Ingen data funnet for skipstype 30\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:39.095732900Z",
     "start_time": "2025-03-13T09:31:59.582327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Opprett en mappe for partisjonerte data\n",
    "output_mappe = \"Data/partitioned_minutes\"\n",
    "os.makedirs(output_mappe, exist_ok=True)\n",
    "\n",
    "# 2. Lag en kopi for å unngå SettingWithCopyWarning\n",
    "data_for_partisjonering = innenfor_område.copy()\n",
    "\n",
    "# 3. Bruk .loc for å legge til 10-minutters intervall kolonne\n",
    "# Dette gir intervaller 0-143 for hele dagen (144 intervaller på 10 minutter)\n",
    "data_for_partisjonering.loc[:, 'minuttgruppe'] = (\n",
    "    data_for_partisjonering['date_time_utc'].dt.hour * 6 + \n",
    "    data_for_partisjonering['date_time_utc'].dt.minute // 10\n",
    ")\n",
    "\n",
    "# 4. Partisjonering basert på minuttgruppe og ship_type\n",
    "for minuttgruppe, gruppe_minutt in data_for_partisjonering.groupby('minuttgruppe'):\n",
    "    # Konverter minuttgruppe til time og minutt for mappe-strukturen\n",
    "    time = minuttgruppe // 6\n",
    "    minutt = (minuttgruppe % 6) * 10\n",
    "    \n",
    "    # Lag en lesbar mappestruktur (time_minutt=HH_MM)\n",
    "    minutt_mappe = os.path.join(output_mappe, f\"time_minutt={time:02d}_{minutt:02d}\")\n",
    "    os.makedirs(minutt_mappe, exist_ok=True)\n",
    "    \n",
    "    for ship_type, gruppe_final in gruppe_minutt.groupby('ship_type'):\n",
    "        skip_mappe = os.path.join(minutt_mappe, f\"ship_type={ship_type}\")\n",
    "        os.makedirs(skip_mappe, exist_ok=True)\n",
    "        \n",
    "        # Bruk en kopi av dataene\n",
    "        data_å_lagre = gruppe_final.copy()\n",
    "        \n",
    "        # Fjern partisjoneringskolonnen før lagring\n",
    "        fil_sti = os.path.join(skip_mappe, f\"data.parquet\")\n",
    "        data_å_lagre.drop('minuttgruppe', axis=1).to_parquet(fil_sti)\n",
    "        \n",
    "        print(f\"Skrevet {len(gruppe_final)} rader til {fil_sti}\")"
   ],
   "id": "32e3a7d0605f2bae",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'innenfor_område' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(output_mappe, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# 2. Lag en kopi for å unngå SettingWithCopyWarning\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m data_for_partisjonering \u001B[38;5;241m=\u001B[39m innenfor_område\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# 3. Bruk .loc for å legge til 10-minutters intervall kolonne\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Dette gir intervaller 0-143 for hele dagen (144 intervaller på 10 minutter)\u001B[39;00m\n\u001B[0;32m     14\u001B[0m data_for_partisjonering\u001B[38;5;241m.\u001B[39mloc[:, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mminuttgruppe\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     15\u001B[0m     data_for_partisjonering[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time_utc\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mhour \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m6\u001B[39m \u001B[38;5;241m+\u001B[39m \n\u001B[0;32m     16\u001B[0m     data_for_partisjonering[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time_utc\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mminute \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m     17\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'innenfor_område' is not defined"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Strømming",
   "id": "5e26783c2e2b85d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:39.095732900Z",
     "start_time": "2025-03-13T09:32:02.873444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Simulert datastrømming over timer\n",
    "partisjon_mappe = \"Data/partitioned_hour\"\n",
    "\n",
    "# Finn alle time-mapper sortert kronologisk\n",
    "alle_timer = sorted([d for d in os.listdir(partisjon_mappe) if d.startswith(\"time=\")])\n",
    "\n",
    "print(\"\\nSimulerer datastrømming over timer...\")\n",
    "for time_dir in alle_timer:\n",
    "    time_verdi = time_dir.split(\"=\")[1]\n",
    "    time_sti = os.path.join(partisjon_mappe, time_dir)\n",
    "    \n",
    "    print(f\"\\nProsesserer data for time {time_verdi}...\")\n",
    "    \n",
    "    # Finn alle geoparquet-filer for denne timen\n",
    "    filer = glob.glob(os.path.join(time_sti, \"**/*.parquet\"), recursive=True)\n",
    "    \n",
    "    # Les og bearbeid hver fil\n",
    "    time_data = []\n",
    "    for fil in filer:\n",
    "        gdf = gpd.read_parquet(fil)\n",
    "        time_data.append(gdf)\n",
    "        \n",
    "    if time_data:\n",
    "        samlet_data = pd.concat(time_data)\n",
    "        print(f\"Time {time_verdi}: Lastet {len(samlet_data)} rader fra {len(filer)} filer\")\n",
    "        \n",
    "        # Her kan du gjøre din analyse for denne timen\n",
    "        # For eksempel:\n",
    "        print(f\"  Gjennomsnittlig hastighet (SOG): {samlet_data['speed_over_ground'].mean():.2f}\")\n",
    "        print(f\"  Antall unike skip: {samlet_data['mmsi'].nunique()}\")\n",
    "        \n",
    "        # For å visualisere bevegelse over tid, kan du sortere på tidspunkt innen timen\n",
    "        samlet_data_sortert = samlet_data.sort_values('date_time_utc')\n",
    "        \n",
    "        # (Her kunne du lagt til kode for visualisering)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Ingen data funnet for time {time_verdi}\")\n",
    "    \n",
    "    # Simuler tid mellom timer\n",
    "    time_module.sleep(1)  # Vent ett sekund mellom hver time\n",
    "\n",
    "print(\"\\nDatastrømming komplett!\")"
   ],
   "id": "10584cdbcd0515aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulerer datastrømming over timer...\n",
      "\n",
      "Datastrømming komplett!\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:39.095732900Z",
     "start_time": "2025-03-13T09:32:06.631196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Simulert datastrømming over 10-minutters intervaller\n",
    "partisjon_mappe = \"Data/partitioned_minutes\"\n",
    "\n",
    "# Finn alle minutt-mapper sortert kronologisk\n",
    "alle_intervaller = sorted([d for d in os.listdir(partisjon_mappe) if d.startswith(\"time_minutt=\")])\n",
    "\n",
    "print(\"\\nSimulerer datastrømming over 10-minutters intervaller...\")\n",
    "for intervall_dir in alle_intervaller:\n",
    "    # Hent time og minutt fra mappenavnet\n",
    "    match = re.search(r\"time_minutt=(\\d{2})_(\\d{2})\", intervall_dir)\n",
    "    if match:\n",
    "        time, minutt = match.groups()\n",
    "        \n",
    "        intervall_sti = os.path.join(partisjon_mappe, intervall_dir)\n",
    "        print(f\"\\nProsesserer data for {time}:{minutt}...\")\n",
    "        \n",
    "        # Finn alle geoparquet-filer for dette intervallet\n",
    "        filer = glob.glob(os.path.join(intervall_sti, \"**/*.parquet\"), recursive=True)\n",
    "        \n",
    "        # Les og bearbeid hver fil\n",
    "        intervall_data = []\n",
    "        for fil in filer:\n",
    "            gdf = gpd.read_parquet(fil)\n",
    "            intervall_data.append(gdf)\n",
    "            \n",
    "        if intervall_data:\n",
    "            samlet_data = pd.concat(intervall_data)\n",
    "            print(f\"Kl. {time}:{minutt}: Lastet {len(samlet_data)} rader fra {len(filer)} filer\")\n",
    "            \n",
    "            # Her kan du gjøre din analyse for dette intervallet\n",
    "            print(f\"  Gjennomsnittlig hastighet (SOG): {samlet_data['speed_over_ground'].mean():.2f}\")\n",
    "            print(f\"  Antall unike skip: {samlet_data['mmsi'].nunique()}\")\n",
    "            \n",
    "            # Sorter data etter nøyaktig tidspunkt innen intervallet\n",
    "            samlet_data_sortert = samlet_data.sort_values('date_time_utc')\n",
    "            \n",
    "            # Her kunne du lagt til mer analyse eller visualisering\n",
    "            # For eksempel se på hvordan skip beveger seg innen dette 10-minutters intervallet\n",
    "            \n",
    "        else:\n",
    "            print(f\"Ingen data funnet for kl. {time}:{minutt}\")\n",
    "        \n",
    "        # Simuler tid mellom intervaller (kortere siden dette er en mer detaljert simulering)\n",
    "        time_module.sleep(0.5)  # Vent et halvt sekund mellom hvert intervall\n",
    "        \n",
    "print(\"\\nDatastrømming komplett!\")"
   ],
   "id": "2f3f847ef32ee13d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulerer datastrømming over 10-minutters intervaller...\n",
      "\n",
      "Datastrømming komplett!\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main",
   "id": "2cd206b5f48b74b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:39.096732400Z",
     "start_time": "2025-03-13T13:04:35.118692Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "fd34780f43e64663",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mappen Data finnes allerede\n",
      "\n",
      "\n",
      "OPPSUMMERING\n",
      "Katalog: C:\\Users\\ANDRI\\Documents\\Bachelor\\Programmering\\kaidata_geolake\\Data\n",
      "Mappestruktur: Raw=C:\\Users\\ANDRI\\Documents\\Bachelor\\Programmering\\kaidata_geolake\\Data\\raw, Processed=C:\\Users\\ANDRI\\Documents\\Bachelor\\Programmering\\kaidata_geolake\\Data\\processed\n",
      "Totalt antall geografiske filer funnet: 0\n",
      "Antall parquet-filer som allerede var GeoParquet: 0\n",
      "Antall filer konvertert til GeoParquet: 0\n",
      "Antall filer som ikke kunne konverteres: 0\n",
      "\n",
      "Totalt konverterte filer: 0\n",
      "Filer som allerede var i GeoParquet-format: 0\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:51:39.096732400Z",
     "start_time": "2025-03-13T09:32:14.266236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## Konvertere én enkelt fil - eksempel\n",
    "\n",
    "# Definerer stier basert på den opprettede mappestrukturen\n",
    "if os.path.exists(ROT_FOLDER):\n",
    "    rå_mappe = os.path.join(ROT_FOLDER, \"raw\")\n",
    "    prosessert_mappe = os.path.join(ROT_FOLDER, \"processed\")\n",
    "\n",
    "    # Eksempelfil - dette er for demonstrasjon,\n",
    "    eksempelfil = os.path.join(rå_mappe, \"veier_kristiansand.geojson\")\n",
    "\n",
    "    # Utfør kun hvis filen eksisterer (endre denne kommentaren hvis du har en faktisk fil)\n",
    "    if os.path.exists(eksempelfil):\n",
    "        print(f\"Konverterer {eksempelfil}...\")\n",
    "        resultat = converter_parquet(eksempelfil)\n",
    "        if resultat:\n",
    "            print(f\"Filen ble konvertert og lagret til: {resultat}\")\n",
    "\n",
    "            # Lese og vise data\n",
    "            try:\n",
    "                gdf = gpd.read_parquet(resultat)\n",
    "                display(gdf.head())\n",
    "\n",
    "                # Enkel kartvisning\n",
    "                import matplotlib.pyplot as plt\n",
    "                fig, ax = plt.subplots(figsize=(10,8))\n",
    "                gdf.plot(ax=ax)\n",
    "                plt.title(\"Konvertert GeoParquet-fil\")\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"Kunne ikke vise data: {e}\")\n",
    "    else:\n",
    "        print(f\"Ingen eksempelfil funnet på {eksempelfil}\")\n",
    "        print(\"For å teste konvertering:\")\n",
    "        print(f\"1. Legg en geografisk fil (f.eks. GeoJSON eller Shapefile) i {rå_mappe}\")\n",
    "        print(f\"2. Kjør denne cellen på nytt med korrekt filsti\")\n",
    "else:\n",
    "    print(f\"Rotmappe {ROT_FOLDER} finnes ikke. Kjør seksjon 8 først.\")"
   ],
   "id": "33efb0c476aacc4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingen eksempelfil funnet på ./data\\raw\\veier_kristiansand.geojson\n",
      "For å teste konvertering:\n",
      "1. Legg en geografisk fil (f.eks. GeoJSON eller Shapefile) i ./data\\raw\n",
      "2. Kjør denne cellen på nytt med korrekt filsti\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "af7c432fd4512b76"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
